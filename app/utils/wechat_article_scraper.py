#!/usr/bin/env python3
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ä¿¡æ¯è‡ªåŠ¨é‡‡é›†å·¥å…·
æ”¯æŒä»å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é“¾æ¥ä¸­æå–å‘å¸ƒä¿¡æ¯
"""

import requests
import re
import json
from datetime import datetime
from typing import Dict, Optional, List
import time
from urllib.parse import urlparse, parse_qs
import streamlit as st

class WeChatArticleScraper:
    """å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ä¿¡æ¯é‡‡é›†å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def extract_article_id(self, url: str) -> Optional[str]:
        """ä»URLä¸­æå–æ–‡ç« ID"""
        try:
            # å¾®ä¿¡å…¬ä¼—å·æ–‡ç« URLæ ¼å¼ï¼šhttps://mp.weixin.qq.com/s/æ–‡ç« ID
            match = re.search(r'/s/([a-zA-Z0-9_-]+)', url)
            if match:
                return match.group(1)
            return None
        except Exception as e:
            print(f"æå–æ–‡ç« IDå¤±è´¥: {e}")
            return None
    
    def get_article_info(self, url: str) -> Optional[Dict]:
        """è·å–æ–‡ç« åŸºæœ¬ä¿¡æ¯"""
        try:
            article_id = self.extract_article_id(url)
            if not article_id:
                return None
            
            # æ„å»ºAPIè¯·æ±‚URL
            api_url = f"https://mp.weixin.qq.com/mp/getappmsgext"
            
            params = {
                '__biz': '',  # éœ€è¦ä»é¡µé¢ä¸­æå–
                'mid': article_id,
                'sn': '',     # éœ€è¦ä»é¡µé¢ä¸­æå–
                'idx': '1',
                'appmsg_type': '9',
                'f': 'json'
            }
            
            response = self.session.get(url)
            if response.status_code == 200:
                # ä»é¡µé¢ä¸­æå–å¿…è¦å‚æ•°
                html_content = response.text
                
                # æå–__bizå‚æ•°
                biz_match = re.search(r'__biz=([^&]+)', html_content)
                if biz_match:
                    params['__biz'] = biz_match.group(1)
                
                # æå–snå‚æ•°
                sn_match = re.search(r'sn=([^&]+)', html_content)
                if sn_match:
                    params['sn'] = sn_match.group(1)
                
                # å°è¯•è·å–æ–‡ç« æ•°æ®
                return self._parse_article_data(html_content, article_id)
            
            return None
            
        except Exception as e:
            print(f"è·å–æ–‡ç« ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def _parse_article_data(self, html_content: str, article_id: str) -> Dict:
        """è§£ææ–‡ç« æ•°æ®"""
        article_info = {
            'id': article_id,
            'url': '',
            'title': '',
            'publish_date': '',
            'publish_time': '',
            'status': 'published',
            'views': 0,
            'likes': 0,
            'comments': 0,
            'shares': 0,
            'channel_name': '',
            'tags': []
        }
        
        try:
            # æå–æ ‡é¢˜
            title_match = re.search(r'<h1[^>]*>([^<]+)</h1>', html_content)
            if title_match:
                article_info['title'] = title_match.group(1).strip()
            
            # æå–å‘å¸ƒæ—¶é—´
            time_match = re.search(r'var publish_time = "([^"]+)"', html_content)
            if time_match:
                publish_time_str = time_match.group(1)
                try:
                    publish_time = datetime.fromtimestamp(int(publish_time_str))
                    article_info['publish_date'] = publish_time.strftime('%Y-%m-%d')
                    article_info['publish_time'] = publish_time.strftime('%H:%M')
                except:
                    pass
            
            # æå–å…¬ä¼—å·åç§°
            account_match = re.search(r'var nickname = "([^"]+)"', html_content)
            if account_match:
                article_info['channel_name'] = account_match.group(1)
            
            # æå–é˜…è¯»é‡ç­‰ä¿¡æ¯ï¼ˆéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
            # æ³¨æ„ï¼šå¾®ä¿¡å…¬ä¼—å·çš„é˜…è¯»é‡ç­‰æ•°æ®éœ€è¦ç‰¹æ®Šæƒé™æ‰èƒ½è·å–
            
        except Exception as e:
            print(f"è§£ææ–‡ç« æ•°æ®å¤±è´¥: {e}")
        
        return article_info
    
    def scrape_from_url(self, url: str) -> Optional[Dict]:
        """ä»URLç›´æ¥é‡‡é›†æ–‡ç« ä¿¡æ¯"""
        try:
            # éªŒè¯URLæ ¼å¼
            if not url.startswith('https://mp.weixin.qq.com/s/'):
                return None
            
            article_info = self.get_article_info(url)
            if article_info:
                article_info['url'] = url
                return article_info
            
            return None
            
        except Exception as e:
            print(f"URLé‡‡é›†å¤±è´¥: {e}")
            return None

class WeChatDataCollector:
    """å¾®ä¿¡å…¬ä¼—å·æ•°æ®é‡‡é›†å™¨"""
    
    def __init__(self, data_collector):
        self.scraper = WeChatArticleScraper()
        self.data_collector = data_collector
    
    def add_article_from_url(self, url: str, channel_name: str = None) -> bool:
        """ä»URLæ·»åŠ æ–‡ç« """
        try:
            article_info = self.scraper.scrape_from_url(url)
            if not article_info:
                return False
            
            # å¦‚æœæä¾›äº†é¢‘é“åç§°ï¼Œä½¿ç”¨æä¾›çš„åç§°
            if channel_name:
                article_info['channel_name'] = channel_name
            
            # å¦‚æœæ²¡æœ‰é¢‘é“åç§°ï¼Œä½¿ç”¨é»˜è®¤åç§°
            if not article_info['channel_name']:
                article_info['channel_name'] = 'æœªçŸ¥å…¬ä¼—å·'
            
            # æ·»åŠ åˆ°æ•°æ®é‡‡é›†å™¨
            self.data_collector.add_publish_record(
                article_info['channel_name'], 
                article_info
            )
            
            return True
            
        except Exception as e:
            print(f"æ·»åŠ æ–‡ç« å¤±è´¥: {e}")
            return False
    
    def batch_add_articles(self, urls: List[str], channel_name: str = None) -> Dict:
        """æ‰¹é‡æ·»åŠ æ–‡ç« """
        results = {
            'success': 0,
            'failed': 0,
            'errors': []
        }
        
        for url in urls:
            try:
                if self.add_article_from_url(url, channel_name):
                    results['success'] += 1
                else:
                    results['failed'] += 1
                    results['errors'].append(f"å¤±è´¥: {url}")
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(1)
                
            except Exception as e:
                results['failed'] += 1
                results['errors'].append(f"é”™è¯¯: {url} - {str(e)}")
        
        return results

def create_manual_entry_form():
    """åˆ›å»ºæ‰‹åŠ¨å½•å…¥è¡¨å•"""
    st.subheader("ğŸ“ æ‰‹åŠ¨å½•å…¥æ–‡ç« ä¿¡æ¯")
    
    with st.form("manual_article_form"):
        url = st.text_input("æ–‡ç« é“¾æ¥", placeholder="https://mp.weixin.qq.com/s/...")
        channel_name = st.text_input("å…¬ä¼—å·åç§°", placeholder="è¯·è¾“å…¥å…¬ä¼—å·åç§°")
        
        col1, col2 = st.columns(2)
        with col1:
            title = st.text_input("æ–‡ç« æ ‡é¢˜")
            publish_date = st.date_input("å‘å¸ƒæ—¥æœŸ", value=datetime.now().date())
        with col2:
            publish_time = st.time_input("å‘å¸ƒæ—¶é—´", value=datetime.now().time())
            status = st.selectbox("çŠ¶æ€", ["published", "draft", "scheduled"])
        
        col1, col2 = st.columns(2)
        with col1:
            views = st.number_input("é˜…è¯»é‡", min_value=0, value=0)
            likes = st.number_input("ç‚¹èµæ•°", min_value=0, value=0)
        with col2:
            comments = st.number_input("è¯„è®ºæ•°", min_value=0, value=0)
            shares = st.number_input("åˆ†äº«æ•°", min_value=0, value=0)
        
        tags_input = st.text_input("æ ‡ç­¾", placeholder="ç”¨é€—å·åˆ†éš”ï¼Œå¦‚: AI,æŠ€æœ¯,æ–°é—»")
        
        if st.form_submit_button("æ·»åŠ æ–‡ç« "):
            if title.strip() and channel_name.strip():
                tags = [tag.strip() for tag in tags_input.split(',') if tag.strip()]
                
                article_info = {
                    'title': title.strip(),
                    'publish_date': publish_date.strftime('%Y-%m-%d'),
                    'publish_time': publish_time.strftime('%H:%M'),
                    'status': status,
                    'views': views,
                    'likes': likes,
                    'comments': comments,
                    'shares': shares,
                    'url': url.strip(),
                    'tags': tags
                }
                
                return channel_name.strip(), article_info
            
            else:
                st.error("è¯·å¡«å†™æ–‡ç« æ ‡é¢˜å’Œå…¬ä¼—å·åç§°")
                return None, None
    
    return None, None

def main():
    """ä¸»å‡½æ•° - æµ‹è¯•é‡‡é›†åŠŸèƒ½"""
    from app.utils.data_collector import ChannelDataCollector
    
    collector = ChannelDataCollector()
    wechat_collector = WeChatDataCollector(collector)
    
    # æµ‹è¯•URL
    test_url = "https://mp.weixin.qq.com/s/YEEgiKCu2YMUls7QFJ24EQ"
    
    print("ğŸ§ª æµ‹è¯•å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é‡‡é›†")
    print("=" * 50)
    
    # å°è¯•é‡‡é›†æ–‡ç« ä¿¡æ¯
    article_info = wechat_collector.scraper.scrape_from_url(test_url)
    
    if article_info:
        print("âœ… æ–‡ç« ä¿¡æ¯é‡‡é›†æˆåŠŸ:")
        for key, value in article_info.items():
            print(f"  {key}: {value}")
    else:
        print("âŒ æ–‡ç« ä¿¡æ¯é‡‡é›†å¤±è´¥")
        print("ğŸ’¡ å¯èƒ½çš„åŸå› :")
        print("  1. æ–‡ç« éœ€è¦éªŒè¯æ‰èƒ½è®¿é—®")
        print("  2. ç½‘ç»œè¿æ¥é—®é¢˜")
        print("  3. å¾®ä¿¡å…¬ä¼—å·åçˆ¬è™«æœºåˆ¶")
        print("  4. éœ€è¦ç™»å½•æˆ–ç‰¹æ®Šæƒé™")

if __name__ == "__main__":
    main() 