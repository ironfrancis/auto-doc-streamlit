#!/usr/bin/env python3
"""
æ²³æµå›¾å¯è§†åŒ–é¡µé¢æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯æ–°åˆ›å»ºçš„æ²³æµå›¾å¯è§†åŒ–é¡µé¢åŠŸèƒ½
"""

import sys
import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºæµ‹è¯•"""
    print("ğŸ“Š åˆ›å»ºç¤ºä¾‹æ•°æ®...")
    
    # åˆ›å»ºç¤ºä¾‹è´¦å·
    accounts = ['AGIè§‚å¯Ÿå®¤', 'AGIå¯ç¤ºå½•', 'AIä¸‡è±¡å¿—', 'äººå·¥æ™ºèƒ½æ¼«æ¸¸æŒ‡å—']
    
    # åˆ›å»ºæ—¶é—´èŒƒå›´ï¼ˆæœ€è¿‘3ä¸ªæœˆï¼‰
    end_date = datetime.now()
    start_date = end_date - timedelta(days=90)
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    data = []
    for i in range(100):  # ç”Ÿæˆ100æ¡è®°å½•
        # éšæœºé€‰æ‹©è´¦å·
        account = np.random.choice(accounts)
        
        # éšæœºç”Ÿæˆæ—¶é—´
        random_days = np.random.randint(0, 90)
        publish_time = start_date + timedelta(days=random_days)
        
        # ç”Ÿæˆé˜…è¯»é‡ï¼ˆåŸºäºè´¦å·çš„åŸºå‡†å€¼ï¼‰
        base_reads = {
            'AGIè§‚å¯Ÿå®¤': 1000,
            'AGIå¯ç¤ºå½•': 800,
            'AIä¸‡è±¡å¿—': 600,
            'äººå·¥æ™ºèƒ½æ¼«æ¸¸æŒ‡å—': 400
        }
        
        # æ·»åŠ éšæœºæ³¢åŠ¨
        reads = base_reads[account] + np.random.randint(-200, 500)
        reads = max(0, reads)  # ç¡®ä¿éè´Ÿ
        
        # ç”Ÿæˆå…¶ä»–æŒ‡æ ‡
        likes = int(reads * np.random.uniform(0.02, 0.08))
        shares = int(reads * np.random.uniform(0.01, 0.05))
        
        data.append({
            'å‘å¸ƒæ—¶é—´': publish_time.strftime('%Y-%m-%d'),
            'é˜…è¯»é‡': reads,
            'ç‚¹èµé‡': likes,
            'åˆ†äº«é‡': shares,
            'è´¦å·åç§°': account,
            'æ ‡é¢˜': f'æµ‹è¯•æ–‡ç« _{i+1}',
            'é“¾æ¥': f'https://example.com/article_{i+1}'
        })
    
    return pd.DataFrame(data)

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½...")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    df = create_sample_data()
    
    # ä¿å­˜åˆ°æµ‹è¯•æ–‡ä»¶
    test_file = "workspace/data/publish_history_for_calendar.csv"
    os.makedirs(os.path.dirname(test_file), exist_ok=True)
    df.to_csv(test_file, index=False, encoding='utf-8-sig')
    
    print(f"âœ… ç¤ºä¾‹æ•°æ®å·²ä¿å­˜åˆ°: {test_file}")
    print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"ğŸ“‹ åˆ—å: {list(df.columns)}")
    print(f"ğŸ“… æ—¶é—´èŒƒå›´: {df['å‘å¸ƒæ—¶é—´'].min()} åˆ° {df['å‘å¸ƒæ—¶é—´'].max()}")
    print(f"ğŸ‘¥ è´¦å·æ•°é‡: {len(df['è´¦å·åç§°'].unique())}")
    
    return df

def test_data_processing():
    """æµ‹è¯•æ•°æ®å¤„ç†åŠŸèƒ½"""
    print("âš™ï¸ æµ‹è¯•æ•°æ®å¤„ç†åŠŸèƒ½...")
    
    # åŠ è½½æ•°æ®
    df = create_sample_data()
    
    # æµ‹è¯•æ—¶é—´è½¬æ¢
    df['å‘å¸ƒæ—¶é—´'] = pd.to_datetime(df['å‘å¸ƒæ—¶é—´'], errors='coerce')
    df = df.dropna(subset=['å‘å¸ƒæ—¶é—´'])
    
    print(f"âœ… æ—¶é—´è½¬æ¢æˆåŠŸï¼Œæœ‰æ•ˆè®°å½•æ•°: {len(df)}")
    
    # æµ‹è¯•æŒ‰æ—¶é—´åˆ†ç»„
    df['æ—¶é—´åˆ†ç»„'] = df['å‘å¸ƒæ—¶é—´'].dt.date
    daily_data = df.groupby(['æ—¶é—´åˆ†ç»„', 'è´¦å·åç§°'])['é˜…è¯»é‡'].sum().reset_index()
    
    print(f"âœ… æŒ‰æ—¥åˆ†ç»„æˆåŠŸï¼Œåˆ†ç»„è®°å½•æ•°: {len(daily_data)}")
    
    # æµ‹è¯•é€è§†è¡¨
    pivot_data = daily_data.pivot(index='æ—¶é—´åˆ†ç»„', columns='è´¦å·åç§°', values='é˜…è¯»é‡').fillna(0)
    
    print(f"âœ… é€è§†è¡¨åˆ›å»ºæˆåŠŸï¼Œå½¢çŠ¶: {pivot_data.shape}")
    
    return pivot_data

def test_visualization_data():
    """æµ‹è¯•å¯è§†åŒ–æ•°æ®å‡†å¤‡"""
    print("ğŸ“ˆ æµ‹è¯•å¯è§†åŒ–æ•°æ®å‡†å¤‡...")
    
    # åŠ è½½æ•°æ®
    df = create_sample_data()
    df['å‘å¸ƒæ—¶é—´'] = pd.to_datetime(df['å‘å¸ƒæ—¶é—´'], errors='coerce')
    df = df.dropna(subset=['å‘å¸ƒæ—¶é—´'])
    
    # æµ‹è¯•ä¸åŒæ—¶é—´ç²’åº¦
    time_granularities = ['daily', 'weekly', 'monthly']
    
    for granularity in time_granularities:
        if granularity == 'daily':
            df['æ—¶é—´åˆ†ç»„'] = df['å‘å¸ƒæ—¶é—´'].dt.date
        elif granularity == 'weekly':
            df['æ—¶é—´åˆ†ç»„'] = df['å‘å¸ƒæ—¶é—´'].dt.to_period('W').dt.start_time.dt.date
        elif granularity == 'monthly':
            df['æ—¶é—´åˆ†ç»„'] = df['å‘å¸ƒæ—¶é—´'].dt.to_period('M').dt.start_time.dt.date
        
        flow_data = df.groupby(['æ—¶é—´åˆ†ç»„', 'è´¦å·åç§°'])['é˜…è¯»é‡'].sum().reset_index()
        pivot_data = flow_data.pivot(index='æ—¶é—´åˆ†ç»„', columns='è´¦å·åç§°', values='é˜…è¯»é‡').fillna(0)
        
        print(f"âœ… {granularity} ç²’åº¦æ•°æ®å¤„ç†æˆåŠŸï¼Œå½¢çŠ¶: {pivot_data.shape}")
    
    return True

def test_page_imports():
    """æµ‹è¯•é¡µé¢å¯¼å…¥åŠŸèƒ½"""
    print("ğŸ“¦ æµ‹è¯•é¡µé¢å¯¼å…¥åŠŸèƒ½...")
    
    pages = [
        'pages.18_Reading_Flow_Chart',
        'pages.19_Advanced_Flow_Visualization', 
        'pages.20_River_Flow_Diagram'
    ]
    
    for page in pages:
        try:
            module = __import__(page, fromlist=['main'])
            print(f"âœ… {page} å¯¼å…¥æˆåŠŸ")
        except ImportError as e:
            print(f"âŒ {page} å¯¼å…¥å¤±è´¥: {e}")
        except Exception as e:
            print(f"âš ï¸ {page} å¯¼å…¥æ—¶å‡ºç°å…¶ä»–é”™è¯¯: {e}")
    
    return True

def test_dependencies():
    """æµ‹è¯•ä¾èµ–åŒ…"""
    print("ğŸ”§ æµ‹è¯•ä¾èµ–åŒ…...")
    
    required_packages = [
        'pandas',
        'numpy',
        'plotly',
        'streamlit',
        'datetime'
    ]
    
    for package in required_packages:
        try:
            __import__(package)
            print(f"âœ… {package} å¯ç”¨")
        except ImportError:
            print(f"âŒ {package} ä¸å¯ç”¨")
    
    return True

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸŒŠ æ²³æµå›¾å¯è§†åŒ–é¡µé¢æµ‹è¯•å¼€å§‹")
    print("=" * 50)
    
    try:
        # æµ‹è¯•ä¾èµ–åŒ…
        test_dependencies()
        print()
        
        # æµ‹è¯•é¡µé¢å¯¼å…¥
        test_page_imports()
        print()
        
        # æµ‹è¯•æ•°æ®åŠ è½½
        test_data_loading()
        print()
        
        # æµ‹è¯•æ•°æ®å¤„ç†
        test_data_processing()
        print()
        
        # æµ‹è¯•å¯è§†åŒ–æ•°æ®å‡†å¤‡
        test_visualization_data()
        print()
        
        print("=" * 50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print()
        print("ğŸ“‹ æµ‹è¯•æ€»ç»“:")
        print("âœ… ä¾èµ–åŒ…æ£€æŸ¥é€šè¿‡")
        print("âœ… é¡µé¢å¯¼å…¥æµ‹è¯•é€šè¿‡")
        print("âœ… æ•°æ®åŠ è½½åŠŸèƒ½æ­£å¸¸")
        print("âœ… æ•°æ®å¤„ç†åŠŸèƒ½æ­£å¸¸")
        print("âœ… å¯è§†åŒ–æ•°æ®å‡†å¤‡æ­£å¸¸")
        print()
        print("ğŸš€ å¯ä»¥å¼€å§‹ä½¿ç”¨æ²³æµå›¾å¯è§†åŒ–åŠŸèƒ½äº†ï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
