#!/usr/bin/env python3
"""
æ¸…ç©ºç¤ºä¾‹æ•°æ®è„šæœ¬
ç”¨äºåˆ é™¤ç¤ºä¾‹æ•°æ®ï¼Œè®©ç”¨æˆ·ä»é›¶å¼€å§‹å½•å…¥çœŸå®æ•°æ®
"""

import json
import os
from datetime import datetime

def backup_current_data():
    """å¤‡ä»½å½“å‰æ•°æ®"""
    data_file = "workspace/data/publish_history.csv"
    if os.path.exists(data_file):
        backup_file = f"workspace/data/json/channel_publish_history_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å½“å‰æ•°æ®å·²å¤‡ä»½åˆ°: {backup_file}")
        return True
    else:
        print("âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        return False

def clear_sample_data():
    """æ¸…ç©ºç¤ºä¾‹æ•°æ®"""
    data_file = "workspace/data/publish_history.csv"
    
    # åˆ›å»ºç©ºçš„CSVæ–‡ä»¶ï¼ˆåªåŒ…å«è¡¨å¤´ï¼‰
    csv_header = "å†…å®¹æ ‡é¢˜,å‘è¡¨æ—¶é—´,æ€»é˜…è¯»äººæ•°,æ€»é˜…è¯»æ¬¡æ•°,æ€»åˆ†äº«äººæ•°,æ€»åˆ†äº«æ¬¡æ•°,é˜…è¯»åå…³æ³¨äººæ•°,é€è¾¾äººæ•°,å…¬ä¼—å·æ¶ˆæ¯é˜…è¯»æ¬¡æ•°,é€è¾¾é˜…è¯»ç‡,é¦–æ¬¡åˆ†äº«æ¬¡æ•°,åˆ†äº«äº§ç”Ÿé˜…è¯»æ¬¡æ•°,é¦–æ¬¡åˆ†äº«ç‡,æ¯æ¬¡åˆ†äº«å¸¦æ¥é˜…è¯»æ¬¡æ•°,é˜…è¯»å®Œæˆç‡,å†…å®¹url,channel_name,publish_date,publish_time,status,likes,comments,id,tags"
    
    # å†™å…¥ç©ºæ•°æ®
    with open(data_file, 'w', encoding='utf-8') as f:
        f.write(csv_header + '\n')
    
    print("âœ… ç¤ºä¾‹æ•°æ®å·²æ¸…ç©º")
    print("ğŸ“ ç°åœ¨æ‚¨å¯ä»¥å¼€å§‹å½•å…¥çœŸå®çš„é¢‘é“å‘å¸ƒæ•°æ®")

def show_backup_info():
    """æ˜¾ç¤ºå¤‡ä»½ä¿¡æ¯"""
    print("\nğŸ“‹ å¤‡ä»½ä¿¡æ¯:")
    backup_files = []
    for file in os.listdir("workspace/data/json"):
        if file.startswith("channel_publish_history_backup_") and file.endswith(".json"):
            backup_files.append(file)
    
    if backup_files:
        for file in sorted(backup_files, reverse=True):
            file_path = os.path.join("workspace/data/json", file)
            file_size = os.path.getsize(file_path)
            file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
            print(f"  ğŸ“„ {file} ({file_size} bytes, {file_time.strftime('%Y-%m-%d %H:%M:%S')})")
    else:
        print("  âŒ æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ—‘ï¸ æ¸…ç©ºç¤ºä¾‹æ•°æ®å·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥å½“å‰æ•°æ®
    data_file = "workspace/data/publish_history.csv"
    if os.path.exists(data_file):
        import pandas as pd
        df = pd.read_csv(data_file, encoding='utf-8')
        
        print(f"ğŸ“Š å½“å‰æ•°æ®ç»Ÿè®¡:")
        print(f"  è®°å½•æ•°: {len(df)}")
        if 'channel_name' in df.columns:
            channels = df['channel_name'].unique()
            print(f"  é¢‘é“æ•°: {len(channels)}")
            print(f"  é¢‘é“åˆ—è¡¨: {list(channels)}")
        
        if len(df) > 0:
            print("\nâš ï¸ è­¦å‘Š: è¿™å°†åˆ é™¤æ‰€æœ‰ç°æœ‰æ•°æ®!")
            confirm = input("ç¡®è®¤è¦æ¸…ç©ºæ•°æ®å—? (è¾“å…¥ 'yes' ç¡®è®¤): ")
            
            if confirm.lower() == 'yes':
                # å¤‡ä»½æ•°æ®
                if backup_current_data():
                    # æ¸…ç©ºæ•°æ®
                    clear_sample_data()
                    show_backup_info()
                    
                    print("\nğŸ‰ æ“ä½œå®Œæˆ!")
                    print("ğŸ’¡ ä¸‹ä¸€æ­¥:")
                    print("  1. è®¿é—® 'æ•°æ®å½•å…¥' é¡µé¢æ·»åŠ æ‚¨çš„é¢‘é“")
                    print("  2. å¼€å§‹å½•å…¥çœŸå®çš„å‘å¸ƒè®°å½•")
                    print("  3. åœ¨ 'é¢‘é“å‘å¸ƒå†å²è®°å½•' é¡µé¢æŸ¥çœ‹æ•ˆæœ")
                else:
                    print("âŒ å¤‡ä»½å¤±è´¥ï¼Œæ“ä½œå·²å–æ¶ˆ")
            else:
                print("âŒ æ“ä½œå·²å–æ¶ˆ")
        else:
            print("âœ… æ•°æ®å·²ç»æ˜¯ç©ºçš„ï¼Œæ— éœ€æ¸…ç©º")
    else:
        print("âœ… æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¯ä»¥å¼€å§‹å½•å…¥æ–°æ•°æ®")

if __name__ == "__main__":
    main() 