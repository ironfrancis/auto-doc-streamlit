#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Excelæ–‡ä»¶åˆ°æ—¥å†å‘å¸ƒå†å²å¤„ç†å™¨
ä¸“é—¨å¤„ç† publish_excel/ ç›®å½•ä¸‹çš„Excelæ–‡ä»¶ï¼Œå†™å…¥åˆ° publish_history_for_calendar.csv
æ”¯æŒå»é‡å’Œå¢é‡æ›´æ–°
"""

import os
import sys
import pandas as pd
from datetime import datetime
import re
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

def clean_dataframe(df):
    """æ¸…ç†DataFrameæ•°æ®"""
    if df.empty:
        return df
    
    # ç§»é™¤å®Œå…¨ç©ºç™½çš„è¡Œ
    df = df.dropna(how='all')
    
    # æ¸…ç†åˆ—å
    df.columns = df.columns.str.strip()
    
    return df

def standardize_columns(df, platform):
    """æ ‡å‡†åŒ–åˆ—åå’Œæ•°æ®ç»“æ„"""
    if df.empty:
        return df
    
    # æ ¹æ®å¹³å°ç¡®å®šè´¦å·åç§°å‰ç¼€
    if "å¤´æ¡" in platform:
        account_prefix = "å¤´æ¡å·-"
    elif "ç™¾å®¶å·" in platform:
        account_prefix = "ç™¾å®¶å·-"
    else:
        account_prefix = ""
    
    # æå–è´¦å·åç§°ï¼ˆä»æ–‡ä»¶åä¸­æå–ï¼‰
    account_name = platform.replace("å¤´æ¡-", "").replace("ç™¾å®¶å·-", "")
    full_account_name = account_prefix + account_name
    
    # åˆ›å»ºæ ‡å‡†åŒ–çš„DataFrame
    standardized_df = pd.DataFrame()
    
    # æ ¹æ®ä¸åŒçš„åˆ—åæ¨¡å¼è¿›è¡Œæ˜ å°„
    column_mapping = {
        # æ ‡é¢˜ç›¸å…³
        'æ ‡é¢˜': ['æ ‡é¢˜', 'title', 'Title', 'æ–‡ç« æ ‡é¢˜', 'æ–‡ç« å'],
        'å‘å¸ƒæ—¶é—´': ['å‘å¸ƒæ—¶é—´', 'publish_time', 'PublishTime', 'å‘å¸ƒæ—¶é—´', 'æ—¥æœŸ', 'Date'],
        'é˜…è¯»é‡': ['é˜…è¯»é‡', 'read_count', 'ReadCount', 'é˜…è¯»æ•°', 'é˜…è¯»', 'views'],
        'ç‚¹èµé‡': ['ç‚¹èµé‡', 'like_count', 'LikeCount', 'ç‚¹èµæ•°', 'ç‚¹èµ', 'likes'],
        'è¯„è®ºé‡': ['è¯„è®ºé‡', 'comment_count', 'CommentCount', 'è¯„è®ºæ•°', 'è¯„è®º', 'comments'],
        'é“¾æ¥': ['é“¾æ¥', 'link', 'Link', 'URL', 'url', 'æ–‡ç« é“¾æ¥']
    }
    
    # å°è¯•æ˜ å°„åˆ—
    for target_col, possible_cols in column_mapping.items():
        found_col = None
        for col in possible_cols:
            if col in df.columns:
                found_col = col
                break
        
        if found_col:
            standardized_df[target_col] = df[found_col]
        else:
            # å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”åˆ—ï¼Œåˆ›å»ºç©ºåˆ—
            standardized_df[target_col] = ""
    
    # æ·»åŠ è´¦å·åç§°åˆ—
    standardized_df['è´¦å·åç§°'] = full_account_name
    
    # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„åˆ—éƒ½å­˜åœ¨
    required_columns = ['æ ‡é¢˜', 'è´¦å·åç§°', 'å‘å¸ƒæ—¶é—´', 'é˜…è¯»é‡', 'ç‚¹èµé‡', 'è¯„è®ºé‡', 'é“¾æ¥']
    for col in required_columns:
        if col not in standardized_df.columns:
            standardized_df[col] = ""
    
    # åªä¿ç•™éœ€è¦çš„åˆ—ï¼Œç¡®ä¿é¡ºåº
    standardized_df = standardized_df[required_columns]
    
    return standardized_df

def clean_title(title):
    """æ¸…ç†æ ‡é¢˜"""
    if pd.isna(title) or title == "":
        return ""
    
    title = str(title).strip()
    # ç§»é™¤å¤šä½™çš„å¼•å·
    title = title.strip('"').strip("'")
    # é™åˆ¶é•¿åº¦
    if len(title) > 100:
        title = title[:100] + "..."
    
    return title

def clean_publish_time(time_str):
    """æ¸…ç†å’Œæ ‡å‡†åŒ–å‘å¸ƒæ—¶é—´"""
    if pd.isna(time_str) or time_str == "":
        return ""
    
    time_str = str(time_str).strip()
    
    # å°è¯•è§£æä¸åŒçš„æ—¶é—´æ ¼å¼
    time_formats = [
        '%Y-%m-%d %H:%M:%S',
        '%Y-%m-%d',
        '%Y/%m/%d %H:%M:%S',
        '%Y/%m/%d',
        '%m/%d/%Y %H:%M:%S',
        '%m/%d/%Y',
        '%d/%m/%Y %H:%M:%S',
        '%d/%m/%Y'
    ]
    
    for fmt in time_formats:
        try:
            dt = datetime.strptime(time_str, fmt)
            return dt.strftime('%Y-%m-%d %H:%M:%S')
        except ValueError:
            continue
    
    # å¦‚æœæ‰€æœ‰æ ¼å¼éƒ½å¤±è´¥ï¼Œè¿”å›åŸå­—ç¬¦ä¸²
    return time_str

def clean_numeric_value(value):
    """æ¸…ç†æ•°å€¼å­—æ®µ"""
    if pd.isna(value) or value == "":
        return 0
    
    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æ¸…ç†
    value_str = str(value).strip()
    
    # ç§»é™¤éæ•°å­—å­—ç¬¦ï¼ˆä¿ç•™è´Ÿå·å’Œå°æ•°ç‚¹ï¼‰
    value_str = re.sub(r'[^\d.-]', '', value_str)
    
    try:
        return int(float(value_str))
    except (ValueError, TypeError):
        return 0

def process_excel_file(file_path):
    """å¤„ç†å•ä¸ªExcelæ–‡ä»¶"""
    try:
        print(f"ğŸ“– æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_path}")
        
        # è¯»å–Excelæ–‡ä»¶ï¼Œå¤„ç†å¯èƒ½çš„æ ¼å¼é—®é¢˜
        df = None
        error_messages = []
        
        # æ–¹æ³•1: ä½¿ç”¨openpyxlå¼•æ“
        try:
            df = pd.read_excel(file_path, engine='openpyxl')
            print(f"âœ… ä½¿ç”¨openpyxlæˆåŠŸè¯»å–æ–‡ä»¶")
        except Exception as e1:
            error_messages.append(f"openpyxl: {str(e1)}")
        
        # æ–¹æ³•2: ä½¿ç”¨xlrdå¼•æ“
        if df is None:
            try:
                df = pd.read_excel(file_path, engine='xlrd')
                print(f"âœ… ä½¿ç”¨xlrdæˆåŠŸè¯»å–æ–‡ä»¶")
            except Exception as e2:
                error_messages.append(f"xlrd: {str(e2)}")
        
        # æ–¹æ³•3: å°è¯•è¯»å–ä¸ºCSVï¼ˆå¦‚æœæ˜¯Excelæ ¼å¼çš„CSVï¼‰
        if df is None:
            try:
                # å…ˆå°è¯•è¯»å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
                import openpyxl
                wb = openpyxl.load_workbook(file_path, data_only=True)
                ws = wb.active
                
                # å°†æ•°æ®è½¬æ¢ä¸ºDataFrame
                data = []
                for row in ws.iter_rows(values_only=True):
                    if any(cell is not None for cell in row):  # è·³è¿‡å®Œå…¨ç©ºç™½çš„è¡Œ
                        data.append(row)
                
                if data:
                    df = pd.DataFrame(data[1:], columns=data[0])  # ç¬¬ä¸€è¡Œä½œä¸ºåˆ—å
                    print(f"âœ… ä½¿ç”¨openpyxl data_onlyæ¨¡å¼æˆåŠŸè¯»å–æ–‡ä»¶")
            except Exception as e3:
                error_messages.append(f"openpyxl data_only: {str(e3)}")
        
        # æ–¹æ³•4: å°è¯•ä½¿ç”¨pandasçš„é»˜è®¤å¼•æ“
        if df is None:
            try:
                df = pd.read_excel(file_path)
                print(f"âœ… ä½¿ç”¨pandasé»˜è®¤å¼•æ“æˆåŠŸè¯»å–æ–‡ä»¶")
            except Exception as e4:
                error_messages.append(f"pandasé»˜è®¤: {str(e4)}")
        
        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
        if df is None:
            print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {file_path}")
            print(f"   å°è¯•çš„æ–¹æ³•: {', '.join(error_messages)}")
            return pd.DataFrame()
        
        if df.empty:
            print(f"âš ï¸ æ–‡ä»¶ {file_path} ä¸ºç©ºï¼Œè·³è¿‡")
            return pd.DataFrame()
        
        print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(df)} è¡Œ")
        
        # æ¸…ç†æ•°æ®
        df = clean_dataframe(df)
        
        # ä»æ–‡ä»¶åæå–å¹³å°ä¿¡æ¯
        filename = os.path.basename(file_path)
        platform = os.path.splitext(filename)[0]
        
        # æ ‡å‡†åŒ–åˆ—
        df = standardize_columns(df, platform)
        
        if df.empty:
            print(f"âš ï¸ æ ‡å‡†åŒ–åæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
            return pd.DataFrame()
        
        # æ¸…ç†å„ä¸ªå­—æ®µ
        df['æ ‡é¢˜'] = df['æ ‡é¢˜'].apply(clean_title)
        df['å‘å¸ƒæ—¶é—´'] = df['å‘å¸ƒæ—¶é—´'].apply(clean_publish_time)
        df['é˜…è¯»é‡'] = df['é˜…è¯»é‡'].apply(clean_numeric_value)
        df['ç‚¹èµé‡'] = df['ç‚¹èµé‡'].apply(clean_numeric_value)
        df['è¯„è®ºé‡'] = df['è¯„è®ºé‡'].apply(clean_numeric_value)
        df['é“¾æ¥'] = df['é“¾æ¥'].fillna("").astype(str).str.strip()
        
        # ç§»é™¤æ ‡é¢˜ä¸ºç©ºçš„è®°å½•
        df = df[df['æ ‡é¢˜'] != ""]
        
        print(f"âœ… å¤„ç†åæ•°æ®: {len(df)} è¡Œ")
        
        return df
        
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}")
        return pd.DataFrame()

def remove_duplicate_records(df):
    """ç§»é™¤é‡å¤è®°å½•"""
    if df.empty:
        return df
        
    # åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦
    df['unique_key'] = df['æ ‡é¢˜'] + '|' + df['å‘å¸ƒæ—¶é—´'] + '|' + df['è´¦å·åç§°'] + '|' + df['é“¾æ¥']
    
    # ç§»é™¤é‡å¤è®°å½•ï¼Œä¿ç•™æœ€æ–°çš„æ•°æ®
    df = df.drop_duplicates(subset=['unique_key'], keep='last')
    
    # ç§»é™¤ä¸´æ—¶åˆ—
    df = df.drop('unique_key', axis=1)
    
    return df

def create_unique_id(df):
    """åˆ›å»ºç»Ÿä¸€çš„å”¯ä¸€æ ‡è¯†ç¬¦"""
    if df.empty:
        return df
    df['unique_id'] = df['æ ‡é¢˜'] + '|' + df['å‘å¸ƒæ—¶é—´'] + '|' + df['è´¦å·åç§°']
    return df

def update_calendar_csv(excel_data):
    """æ›´æ–°æ—¥å†å‘å¸ƒå†å²CSVæ–‡ä»¶"""
    csv_path = project_root / "workspace" / "data" / "publish_history_for_calendar.csv"
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    csv_path.parent.mkdir(parents=True, exist_ok=True)
    
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œåˆ™åˆå§‹åŒ–
    if not csv_path.exists() or csv_path.stat().st_size == 0:
        columns = ["æ ‡é¢˜", "è´¦å·åç§°", "å‘å¸ƒæ—¶é—´", "é˜…è¯»é‡", "ç‚¹èµé‡", "è¯„è®ºé‡", "é“¾æ¥"]
        empty_df = pd.DataFrame(columns=columns)
        empty_df.to_csv(csv_path, index=False, encoding="utf-8-sig")
        print("ğŸ“ åˆ›å»ºæ–°çš„CSVæ–‡ä»¶")
    
    # è¯»å–ç°æœ‰æ•°æ®
    try:
        existing_df = pd.read_csv(csv_path, encoding="utf-8-sig")
        if existing_df.empty:
            print("ğŸ“ ç°æœ‰æ•°æ®ä¸ºç©ºï¼Œç›´æ¥ä¿å­˜æ–°æ•°æ®")
            combined_df = excel_data
        else:
            print(f"ğŸ“š ç°æœ‰æ•°æ®: {len(existing_df)} æ¡")
            
            # ä¸ºä¸¤ä¸ªæ•°æ®é›†åˆ›å»ºç»Ÿä¸€çš„å”¯ä¸€æ ‡è¯†ç¬¦
            existing_df = create_unique_id(existing_df)
            excel_data = create_unique_id(excel_data)
            
            # æ‰¾å‡ºéœ€è¦æ–°å¢çš„è®°å½•
            new_records = excel_data[~excel_data['unique_id'].isin(existing_df['unique_id'])]
            print(f"â• å‘ç° {len(new_records)} æ¡æ–°è®°å½•")
            
            # æ‰¾å‡ºéœ€è¦æ›´æ–°çš„è®°å½•
            existing_records = excel_data[excel_data['unique_id'].isin(existing_df['unique_id'])]
            print(f"ğŸ”„ å‘ç° {len(existing_records)} æ¡éœ€è¦æ›´æ–°çš„è®°å½•")
            
            if not existing_records.empty:
                # æ›´æ–°å·²å­˜åœ¨çš„è®°å½•
                for _, new_row in existing_records.iterrows():
                    mask = existing_df['unique_id'] == new_row['unique_id']
                    existing_df.loc[mask, ['é˜…è¯»é‡', 'ç‚¹èµé‡', 'è¯„è®ºé‡']] = [
                        new_row['é˜…è¯»é‡'], 
                        new_row['ç‚¹èµé‡'], 
                        new_row['è¯„è®ºé‡']
                    ]
                print("âœ… å·²æ›´æ–°ç°æœ‰è®°å½•çš„æ•°æ®")
            
            # åˆå¹¶æ–°è®°å½•å’Œæ›´æ–°åçš„æ—§è®°å½•
            if not new_records.empty:
                # ç§»é™¤unique_idåˆ—
                new_records = new_records.drop('unique_id', axis=1)
                existing_df = existing_df.drop('unique_id', axis=1)
                combined_df = pd.concat([existing_df, new_records], ignore_index=True)
                print(f"ğŸ”— åˆå¹¶åå…± {len(combined_df)} æ¡è®°å½•")
            else:
                combined_df = existing_df.drop('unique_id', axis=1)
                print("â„¹ï¸ æ²¡æœ‰æ–°è®°å½•éœ€è¦æ·»åŠ ")
                
    except pd.errors.EmptyDataError:
        print("ğŸ“ ç°æœ‰æ•°æ®ä¸ºç©ºï¼Œç›´æ¥ä¿å­˜æ–°æ•°æ®")
        combined_df = excel_data
    
    # æœ€ç»ˆå»é‡
    before_dedup = len(combined_df)
    combined_df = combined_df.drop_duplicates(subset=['æ ‡é¢˜', 'å‘å¸ƒæ—¶é—´', 'è´¦å·åç§°'], keep='last')
    after_dedup = len(combined_df)
    
    if before_dedup != after_dedup:
        print(f"ğŸ§¹ æœ€ç»ˆå»é‡ï¼šç§»é™¤ {before_dedup - after_dedup} æ¡é‡å¤è®°å½•")
    
    # ä½¿ç”¨è‡ªå®šä¹‰å»é‡å‡½æ•°è¿›è¡Œæœ€ç»ˆæ¸…ç†
    before_final_dedup = len(combined_df)
    combined_df = remove_duplicate_records(combined_df)
    after_final_dedup = len(combined_df)
    
    if before_final_dedup != after_final_dedup:
        print(f"ğŸ§½ æœ€ç»ˆæ¸…ç†ï¼šç§»é™¤ {before_final_dedup - after_final_dedup} æ¡é‡å¤è®°å½•")
    
    # æŒ‰å‘å¸ƒæ—¶é—´æ’åº
    if not combined_df.empty:
        combined_df['å‘å¸ƒæ—¶é—´'] = pd.to_datetime(combined_df['å‘å¸ƒæ—¶é—´'], errors='coerce')
        combined_df = combined_df.sort_values('å‘å¸ƒæ—¶é—´', ascending=False)
        combined_df['å‘å¸ƒæ—¶é—´'] = combined_df['å‘å¸ƒæ—¶é—´'].dt.strftime('%Y-%m-%d %H:%M:%S')
    
    # ä¿å­˜æ•°æ®
    combined_df.to_csv(csv_path, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° {csv_path}ï¼Œå…± {len(combined_df)} æ¡è®°å½•")
    
    return True

def process_all_excel_files():
    """å¤„ç†æ‰€æœ‰Excelæ–‡ä»¶"""
    excel_dir = project_root / "scripts" / "publish_excel"
    
    if not excel_dir.exists():
        print(f"âŒ Excelç›®å½•ä¸å­˜åœ¨: {excel_dir}")
        return False
    
    # è·å–æ‰€æœ‰Excelæ–‡ä»¶
    excel_files = list(excel_dir.glob("*.xlsx")) + list(excel_dir.glob("*.xls"))
    
    if not excel_files:
        print("âš ï¸ æœªæ‰¾åˆ°Excelæ–‡ä»¶")
        return False
    
    print(f"ğŸ“ æ‰¾åˆ° {len(excel_files)} ä¸ªExcelæ–‡ä»¶")
    
    all_data = []
    
    for excel_file in excel_files:
        df = process_excel_file(excel_file)
        if not df.empty:
            all_data.append(df)
    
    if not all_data:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®")
        return False
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    combined_data = pd.concat(all_data, ignore_index=True)
    print(f"ğŸ“Š åˆå¹¶åæ€»æ•°æ®: {len(combined_data)} æ¡")
    
    # å»é‡
    combined_data = remove_duplicate_records(combined_data)
    print(f"ğŸ” å»é‡åæ•°æ®: {len(combined_data)} æ¡")
    
    # æ›´æ–°CSVæ–‡ä»¶
    success = update_calendar_csv(combined_data)
    
    if success:
        print("ğŸ‰ æ‰€æœ‰Excelæ–‡ä»¶å¤„ç†å®Œæˆï¼")
    else:
        print("âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")
    
    return success

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¤„ç†Excelæ–‡ä»¶...")
    print("=" * 50)
    
    success = process_all_excel_files()
    
    print("=" * 50)
    if success:
        print("âœ… å¤„ç†å®Œæˆï¼")
    else:
        print("âŒ å¤„ç†å¤±è´¥ï¼")
    
    return success

if __name__ == "__main__":
    main()
