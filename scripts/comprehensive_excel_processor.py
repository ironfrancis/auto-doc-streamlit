#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆExcelå¤„ç†å™¨
å¤„ç†æ‰€æœ‰ç±»å‹çš„æ–‡ä»¶ï¼šæ­£å¸¸Excelã€ä¿®å¤Excelã€æ¢å¤CSV
"""

import os
import sys
import pandas as pd
from pathlib import Path
import zipfile
import xml.etree.ElementTree as ET

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

def read_excel_with_zipfile(file_path):
    """ä½¿ç”¨zipfileç›´æ¥è¯»å–Excelæ–‡ä»¶å†…å®¹"""
    try:
        with zipfile.ZipFile(file_path, 'r') as zip_file:
            # è¯»å–å…±äº«å­—ç¬¦ä¸²è¡¨
            shared_strings = []
            try:
                with zip_file.open('xl/sharedStrings.xml') as f:
                    tree = ET.parse(f)
                    root = tree.getroot()
                    for si in root.findall('.//{http://schemas.openxmlformats.org/spreadsheetml/2006/main}si'):
                        text_elements = si.findall('.//{http://schemas.openxmlformats.org/spreadsheetml/2006/main}t')
                        if text_elements:
                            text = ''.join([elem.text or '' for elem in text_elements])
                            shared_strings.append(text)
                        else:
                            shared_strings.append('')
            except:
                pass
            
            # è¯»å–å·¥ä½œè¡¨æ•°æ®
            with zip_file.open('xl/worksheets/sheet1.xml') as f:
                tree = ET.parse(f)
                root = tree.getroot()
                
                # è§£æè¡Œæ•°æ®
                rows = []
                for row in root.findall('.//{http://schemas.openxmlformats.org/spreadsheetml/2006/main}row'):
                    row_data = []
                    for cell in row.findall('.//{http://schemas.openxmlformats.org/spreadsheetml/2006/main}c'):
                        cell_type = cell.get('t', '')
                        value_elem = cell.find('.//{http://schemas.openxmlformats.org/spreadsheetml/2006/main}v')
                        
                        if value_elem is not None:
                            value = value_elem.text
                            if cell_type == 's' and value:  # å…±äº«å­—ç¬¦ä¸²
                                try:
                                    idx = int(value)
                                    if idx < len(shared_strings):
                                        row_data.append(shared_strings[idx])
                                    else:
                                        row_data.append('')
                                except:
                                    row_data.append(value)
                            else:
                                row_data.append(value)
                        else:
                            row_data.append('')
                    
                    if any(cell for cell in row_data):  # åªæ·»åŠ éç©ºè¡Œ
                        rows.append(row_data)
                
                if rows:
                    df = pd.DataFrame(rows[1:], columns=rows[0])
                    return df
                else:
                    return pd.DataFrame()
                    
    except Exception as e:
        print(f"âŒ zipfileè¯»å–å¤±è´¥: {str(e)}")
        return pd.DataFrame()

def process_file(file_path):
    """å¤„ç†å•ä¸ªæ–‡ä»¶"""
    try:
        print(f"ğŸ“– æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_path}")
        
        df = None
        file_ext = file_path.suffix.lower()
        
        if file_ext == '.csv':
            # ç›´æ¥è¯»å–CSVæ–‡ä»¶
            try:
                df = pd.read_csv(file_path, encoding='utf-8-sig')
                print(f"âœ… ä½¿ç”¨pandasæˆåŠŸè¯»å–CSVæ–‡ä»¶")
            except:
                try:
                    df = pd.read_csv(file_path, encoding='gbk')
                    print(f"âœ… ä½¿ç”¨GBKç¼–ç æˆåŠŸè¯»å–CSVæ–‡ä»¶")
                except Exception as e:
                    print(f"âŒ æ— æ³•è¯»å–CSVæ–‡ä»¶: {str(e)}")
                    return pd.DataFrame()
        
        elif file_ext in ['.xlsx', '.xls']:
            # å°è¯•å¤šç§æ–¹æ³•è¯»å–Excelæ–‡ä»¶
            error_messages = []
            
            # æ–¹æ³•1: ä½¿ç”¨openpyxlå¼•æ“
            if df is None:
                try:
                    df = pd.read_excel(file_path, engine='openpyxl')
                    print(f"âœ… ä½¿ç”¨openpyxlæˆåŠŸè¯»å–æ–‡ä»¶")
                except Exception as e1:
                    error_messages.append(f"openpyxl: {str(e1)}")
            
            # æ–¹æ³•2: ä½¿ç”¨xlrdå¼•æ“
            if df is None:
                try:
                    df = pd.read_excel(file_path, engine='xlrd')
                    print(f"âœ… ä½¿ç”¨xlrdæˆåŠŸè¯»å–æ–‡ä»¶")
                except Exception as e2:
                    error_messages.append(f"xlrd: {str(e2)}")
            
            # æ–¹æ³•3: ä½¿ç”¨zipfileç›´æ¥è¯»å–
            if df is None:
                try:
                    df = read_excel_with_zipfile(file_path)
                    if not df.empty:
                        print(f"âœ… ä½¿ç”¨zipfileæˆåŠŸè¯»å–æ–‡ä»¶")
                except Exception as e3:
                    error_messages.append(f"zipfile: {str(e3)}")
            
            # æ–¹æ³•4: ä½¿ç”¨pandasé»˜è®¤å¼•æ“
            if df is None:
                try:
                    df = pd.read_excel(file_path)
                    print(f"âœ… ä½¿ç”¨pandasé»˜è®¤å¼•æ“æˆåŠŸè¯»å–æ–‡ä»¶")
                except Exception as e4:
                    error_messages.append(f"pandasé»˜è®¤: {str(e4)}")
            
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥
            if df is None:
                print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {file_path}")
                print(f"   å°è¯•çš„æ–¹æ³•: {', '.join(error_messages)}")
                return pd.DataFrame()
        
        if df.empty:
            print(f"âš ï¸ æ–‡ä»¶ {file_path} ä¸ºç©ºï¼Œè·³è¿‡")
            return pd.DataFrame()
        
        print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(df)} è¡Œ")
        return df
        
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}")
        return pd.DataFrame()

def clean_dataframe(df):
    """æ¸…ç†DataFrameæ•°æ®"""
    if df.empty:
        return df
    
    # ç§»é™¤å®Œå…¨ç©ºç™½çš„è¡Œ
    df = df.dropna(how='all')
    
    # æ¸…ç†åˆ—å
    df.columns = df.columns.str.strip()
    
    return df

def standardize_columns(df, platform):
    """æ ‡å‡†åŒ–åˆ—åå’Œæ•°æ®ç»“æ„"""
    if df.empty:
        return df
    
    # æ ¹æ®å¹³å°ç¡®å®šè´¦å·åç§°å‰ç¼€
    if "å¤´æ¡å·" in platform:
        account_prefix = "å¤´æ¡å·-"
    elif "ç™¾å®¶å·" in platform:
        account_prefix = "ç™¾å®¶å·-"
    else:
        account_prefix = ""
    
    # æå–è´¦å·åç§°ï¼ˆä»æ–‡ä»¶åä¸­æå–ï¼‰
    account_name = platform.replace("å¤´æ¡å·-", "").replace("ç™¾å®¶å·-", "")
    full_account_name = account_prefix + account_name
    
    # åˆ›å»ºæ ‡å‡†åŒ–çš„DataFrame
    standardized_df = pd.DataFrame()
    
    # æ ¹æ®ä¸åŒçš„åˆ—åæ¨¡å¼è¿›è¡Œæ˜ å°„
    column_mapping = {
        # æ ‡é¢˜ç›¸å…³
        'æ ‡é¢˜': ['æ ‡é¢˜', 'title', 'Title', 'æ–‡ç« æ ‡é¢˜', 'æ–‡ç« å'],
        'å‘å¸ƒæ—¶é—´': ['å‘å¸ƒæ—¶é—´', 'publish_time', 'PublishTime', 'å‘å¸ƒæ—¶é—´', 'æ—¥æœŸ', 'Date'],
        'é˜…è¯»é‡': ['é˜…è¯»é‡', 'read_count', 'ReadCount', 'é˜…è¯»æ•°', 'é˜…è¯»', 'views'],
        'ç‚¹èµé‡': ['ç‚¹èµé‡', 'like_count', 'LikeCount', 'ç‚¹èµæ•°', 'ç‚¹èµ', 'likes'],
        'è¯„è®ºé‡': ['è¯„è®ºé‡', 'comment_count', 'CommentCount', 'è¯„è®ºæ•°', 'è¯„è®º', 'comments'],
        'é“¾æ¥': ['é“¾æ¥', 'link', 'Link', 'URL', 'url', 'æ–‡ç« é“¾æ¥']
    }
    
    # å°è¯•æ˜ å°„åˆ—
    for target_col, possible_cols in column_mapping.items():
        found_col = None
        for col in possible_cols:
            if col in df.columns:
                found_col = col
                break
        
        if found_col:
            standardized_df[target_col] = df[found_col]
        else:
            # å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”åˆ—ï¼Œåˆ›å»ºç©ºåˆ—
            standardized_df[target_col] = ""
    
    # æ·»åŠ è´¦å·åç§°åˆ—
    standardized_df['è´¦å·åç§°'] = full_account_name
    
    # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„åˆ—éƒ½å­˜åœ¨
    required_columns = ['æ ‡é¢˜', 'è´¦å·åç§°', 'å‘å¸ƒæ—¶é—´', 'é˜…è¯»é‡', 'ç‚¹èµé‡', 'è¯„è®ºé‡', 'é“¾æ¥']
    for col in required_columns:
        if col not in standardized_df.columns:
            standardized_df[col] = ""
    
    # åªä¿ç•™éœ€è¦çš„åˆ—ï¼Œç¡®ä¿é¡ºåº
    standardized_df = standardized_df[required_columns]
    
    return standardized_df

def clean_title(title):
    """æ¸…ç†æ ‡é¢˜"""
    if pd.isna(title) or title == "":
        return ""
    
    title = str(title).strip()
    # ç§»é™¤å¤šä½™çš„å¼•å·
    title = title.strip('"').strip("'")
    # é™åˆ¶é•¿åº¦
    if len(title) > 100:
        title = title[:100] + "..."
    
    return title

def clean_publish_time(time_str):
    """æ¸…ç†å’Œæ ‡å‡†åŒ–å‘å¸ƒæ—¶é—´"""
    if pd.isna(time_str) or time_str == "":
        return ""
    
    time_str = str(time_str).strip()
    
    # å°è¯•è§£æä¸åŒçš„æ—¶é—´æ ¼å¼
    time_formats = [
        '%Y-%m-%d %H:%M:%S',
        '%Y-%m-%d',
        '%Y/%m/%d %H:%M:%S',
        '%Y/%m/%d',
        '%m/%d/%Y %H:%M:%S',
        '%m/%d/%Y',
        '%d/%m/%Y %H:%M:%S',
        '%d/%m/%Y'
    ]
    
    for fmt in time_formats:
        try:
            from datetime import datetime
            dt = datetime.strptime(time_str, fmt)
            return dt.strftime('%Y-%m-%d %H:%M:%S')
        except ValueError:
            continue
    
    # å¦‚æœæ‰€æœ‰æ ¼å¼éƒ½å¤±è´¥ï¼Œè¿”å›åŸå­—ç¬¦ä¸²
    return time_str

def clean_numeric_value(value):
    """æ¸…ç†æ•°å€¼å­—æ®µ"""
    if pd.isna(value) or value == "":
        return 0
    
    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æ¸…ç†
    value_str = str(value).strip()
    
    # ç§»é™¤éæ•°å­—å­—ç¬¦ï¼ˆä¿ç•™è´Ÿå·å’Œå°æ•°ç‚¹ï¼‰
    import re
    value_str = re.sub(r'[^\d.-]', '', value_str)
    
    try:
        return int(float(value_str))
    except (ValueError, TypeError):
        return 0

def remove_duplicate_records(df):
    """ç§»é™¤é‡å¤è®°å½•"""
    if df.empty:
        return df
        
    # åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦
    df['unique_key'] = df['æ ‡é¢˜'] + '|' + df['å‘å¸ƒæ—¶é—´'] + '|' + df['è´¦å·åç§°'] + '|' + df['é“¾æ¥']
    
    # ç§»é™¤é‡å¤è®°å½•ï¼Œä¿ç•™æœ€æ–°çš„æ•°æ®
    df = df.drop_duplicates(subset=['unique_key'], keep='last')
    
    # ç§»é™¤ä¸´æ—¶åˆ—
    df = df.drop('unique_key', axis=1)
    
    return df

def create_unique_id(df):
    """åˆ›å»ºç»Ÿä¸€çš„å”¯ä¸€æ ‡è¯†ç¬¦"""
    if df.empty:
        return df
    df['unique_id'] = df['æ ‡é¢˜'] + '|' + df['å‘å¸ƒæ—¶é—´'] + '|' + df['è´¦å·åç§°']
    return df

def update_calendar_csv(excel_data):
    """æ›´æ–°æ—¥å†å‘å¸ƒå†å²CSVæ–‡ä»¶"""
    csv_path = project_root / "workspace" / "data" / "publish_history_for_calendar.csv"
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    csv_path.parent.mkdir(parents=True, exist_ok=True)
    
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œåˆ™åˆå§‹åŒ–
    if not csv_path.exists() or csv_path.stat().st_size == 0:
        columns = ["æ ‡é¢˜", "è´¦å·åç§°", "å‘å¸ƒæ—¶é—´", "é˜…è¯»é‡", "ç‚¹èµé‡", "è¯„è®ºé‡", "é“¾æ¥"]
        empty_df = pd.DataFrame(columns=columns)
        empty_df.to_csv(csv_path, index=False, encoding="utf-8-sig")
        print("ğŸ“ åˆ›å»ºæ–°çš„CSVæ–‡ä»¶")
    
    # è¯»å–ç°æœ‰æ•°æ®
    try:
        existing_df = pd.read_csv(csv_path, encoding="utf-8-sig")
        if existing_df.empty:
            print("ğŸ“ ç°æœ‰æ•°æ®ä¸ºç©ºï¼Œç›´æ¥ä¿å­˜æ–°æ•°æ®")
            combined_df = excel_data
        else:
            print(f"ğŸ“š ç°æœ‰æ•°æ®: {len(existing_df)} æ¡")
            
            # ä¸ºä¸¤ä¸ªæ•°æ®é›†åˆ›å»ºç»Ÿä¸€çš„å”¯ä¸€æ ‡è¯†ç¬¦
            existing_df = create_unique_id(existing_df)
            excel_data = create_unique_id(excel_data)
            
            # æ‰¾å‡ºéœ€è¦æ–°å¢çš„è®°å½•
            new_records = excel_data[~excel_data['unique_id'].isin(existing_df['unique_id'])]
            print(f"â• å‘ç° {len(new_records)} æ¡æ–°è®°å½•")
            
            # æ‰¾å‡ºéœ€è¦æ›´æ–°çš„è®°å½•
            existing_records = excel_data[excel_data['unique_id'].isin(existing_df['unique_id'])]
            print(f"ğŸ”„ å‘ç° {len(existing_records)} æ¡éœ€è¦æ›´æ–°çš„è®°å½•")
            
            if not existing_records.empty:
                # æ›´æ–°å·²å­˜åœ¨çš„è®°å½•
                for _, new_row in existing_records.iterrows():
                    mask = existing_df['unique_id'] == new_row['unique_id']
                    existing_df.loc[mask, ['é˜…è¯»é‡', 'ç‚¹èµé‡', 'è¯„è®ºé‡']] = [
                        new_row['é˜…è¯»é‡'], 
                        new_row['ç‚¹èµé‡'], 
                        new_row['è¯„è®ºé‡']
                    ]
                print("âœ… å·²æ›´æ–°ç°æœ‰è®°å½•çš„æ•°æ®")
            
            # åˆå¹¶æ–°è®°å½•å’Œæ›´æ–°åçš„æ—§è®°å½•
            if not new_records.empty:
                # ç§»é™¤unique_idåˆ—
                new_records = new_records.drop('unique_id', axis=1)
                existing_df = existing_df.drop('unique_id', axis=1)
                combined_df = pd.concat([existing_df, new_records], ignore_index=True)
                print(f"ğŸ”— åˆå¹¶åå…± {len(combined_df)} æ¡è®°å½•")
            else:
                combined_df = existing_df.drop('unique_id', axis=1)
                print("â„¹ï¸ æ²¡æœ‰æ–°è®°å½•éœ€è¦æ·»åŠ ")
                
    except pd.errors.EmptyDataError:
        print("ğŸ“ ç°æœ‰æ•°æ®ä¸ºç©ºï¼Œç›´æ¥ä¿å­˜æ–°æ•°æ®")
        combined_df = excel_data
    
    # æœ€ç»ˆå»é‡
    before_dedup = len(combined_df)
    combined_df = combined_df.drop_duplicates(subset=['æ ‡é¢˜', 'å‘å¸ƒæ—¶é—´', 'è´¦å·åç§°'], keep='last')
    after_dedup = len(combined_df)
    
    if before_dedup != after_dedup:
        print(f"ğŸ§¹ æœ€ç»ˆå»é‡ï¼šç§»é™¤ {before_dedup - after_dedup} æ¡é‡å¤è®°å½•")
    
    # ä½¿ç”¨è‡ªå®šä¹‰å»é‡å‡½æ•°è¿›è¡Œæœ€ç»ˆæ¸…ç†
    before_final_dedup = len(combined_df)
    combined_df = remove_duplicate_records(combined_df)
    after_final_dedup = len(combined_df)
    
    if before_final_dedup != after_final_dedup:
        print(f"ğŸ§½ æœ€ç»ˆæ¸…ç†ï¼šç§»é™¤ {before_final_dedup - after_final_dedup} æ¡é‡å¤è®°å½•")
    
    # æŒ‰å‘å¸ƒæ—¶é—´æ’åº
    if not combined_df.empty:
        combined_df['å‘å¸ƒæ—¶é—´'] = pd.to_datetime(combined_df['å‘å¸ƒæ—¶é—´'], errors='coerce')
        combined_df = combined_df.sort_values('å‘å¸ƒæ—¶é—´', ascending=False)
        combined_df['å‘å¸ƒæ—¶é—´'] = combined_df['å‘å¸ƒæ—¶é—´'].dt.strftime('%Y-%m-%d %H:%M:%S')
    
    # ä¿å­˜æ•°æ®
    combined_df.to_csv(csv_path, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° {csv_path}ï¼Œå…± {len(combined_df)} æ¡è®°å½•")
    
    return True

def process_all_files():
    """å¤„ç†æ‰€æœ‰æ–‡ä»¶"""
    # å®šä¹‰è¦å¤„ç†çš„ç›®å½•
    directories = [
        "scripts/publish_excel",           # åŸå§‹Excelæ–‡ä»¶
        "scripts/publish_excel_fixed",     # ä¿®å¤çš„Excelæ–‡ä»¶
        "scripts/publish_excel_recovered", # æ¢å¤çš„CSVæ–‡ä»¶
        "scripts/publish_excel_csv"        # è½¬æ¢çš„CSVæ–‡ä»¶
    ]
    
    all_files = []
    
    # æ”¶é›†æ‰€æœ‰æ–‡ä»¶
    for dir_path in directories:
        dir_obj = Path(dir_path)
        if dir_obj.exists():
            files = list(dir_obj.glob("*.xlsx")) + list(dir_obj.glob("*.xls")) + list(dir_obj.glob("*.csv"))
            all_files.extend(files)
    
    if not all_files:
        print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶")
        return False
    
    print(f"ğŸ“ æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶")
    
    all_data = []
    
    for file_path in all_files:
        # å¤„ç†æ–‡ä»¶
        df = process_file(file_path)
        
        if df.empty:
            continue
        
        # æ¸…ç†æ•°æ®
        df = clean_dataframe(df)
        
        # ä»æ–‡ä»¶åæå–å¹³å°ä¿¡æ¯
        filename = file_path.name
        platform = file_path.stem
        
        # æ ‡å‡†åŒ–åˆ—
        df = standardize_columns(df, platform)
        
        if df.empty:
            continue
        
        # æ¸…ç†å„ä¸ªå­—æ®µ
        df['æ ‡é¢˜'] = df['æ ‡é¢˜'].apply(clean_title)
        df['å‘å¸ƒæ—¶é—´'] = df['å‘å¸ƒæ—¶é—´'].apply(clean_publish_time)
        df['é˜…è¯»é‡'] = df['é˜…è¯»é‡'].apply(clean_numeric_value)
        df['ç‚¹èµé‡'] = df['ç‚¹èµé‡'].apply(clean_numeric_value)
        df['è¯„è®ºé‡'] = df['è¯„è®ºé‡'].apply(clean_numeric_value)
        df['é“¾æ¥'] = df['é“¾æ¥'].fillna("").astype(str).str.strip()
        
        # ç§»é™¤æ ‡é¢˜ä¸ºç©ºçš„è®°å½•
        df = df[df['æ ‡é¢˜'] != ""]
        
        if not df.empty:
            print(f"âœ… å¤„ç†åæ•°æ®: {len(df)} è¡Œ")
            all_data.append(df)
    
    if not all_data:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®")
        return False
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    combined_data = pd.concat(all_data, ignore_index=True)
    print(f"ğŸ“Š åˆå¹¶åæ€»æ•°æ®: {len(combined_data)} æ¡")
    
    # å»é‡
    combined_data = remove_duplicate_records(combined_data)
    print(f"ğŸ” å»é‡åæ•°æ®: {len(combined_data)} æ¡")
    
    # æ›´æ–°CSVæ–‡ä»¶
    success = update_calendar_csv(combined_data)
    
    if success:
        print("ğŸ‰ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼")
    else:
        print("âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")
    
    return success

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç»¼åˆExcelæ•°æ®å¤„ç†å·¥å…·")
    print("=" * 50)
    print("ğŸ“ å¤„ç†ç›®å½•:")
    print("  - scripts/publish_excel/ (åŸå§‹Excelæ–‡ä»¶)")
    print("  - scripts/publish_excel_fixed/ (ä¿®å¤çš„Excelæ–‡ä»¶)")
    print("  - scripts/publish_excel_recovered/ (æ¢å¤çš„CSVæ–‡ä»¶)")
    print("  - scripts/publish_excel_csv/ (è½¬æ¢çš„CSVæ–‡ä»¶)")
    print("ğŸ“„ è¾“å‡ºæ–‡ä»¶: workspace/data/publish_history_for_calendar.csv")
    print("=" * 50)
    
    try:
        success = process_all_files()
        
        if success:
            print("\nâœ… å¤„ç†å®Œæˆï¼")
            print("ğŸ“Š æ•°æ®å·²æ›´æ–°åˆ° publish_history_for_calendar.csv")
        else:
            print("\nâŒ å¤„ç†å¤±è´¥ï¼")
            print("è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶é‡è¯•")
            
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        print("è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œæƒé™")

if __name__ == "__main__":
    main()
