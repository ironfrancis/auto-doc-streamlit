# 今日头条数据去重系统

## 概述

优化后的去重系统确保每次写入数据时都进行完善的重复检查，避免数据重复写入。

## 去重策略

### 1. 多层去重机制

#### 第一层：新数据内部去重
- **时机**：获取新数据后立即进行
- **方法**：使用 `remove_duplicate_records()` 函数
- **标识符**：`标题 + 发布时间 + 账号名称 + 链接`
- **目的**：清理API返回数据中的重复项

#### 第二层：新旧数据对比去重
- **时机**：与现有数据合并前
- **方法**：使用 `create_unique_id()` 函数创建统一标识符
- **标识符**：`标题 + 发布时间 + 账号名称`
- **目的**：识别真正的新记录和需要更新的记录

#### 第三层：最终去重
- **时机**：数据合并后
- **方法**：使用pandas的 `drop_duplicates()`
- **标识符**：`标题 + 发布时间 + 账号名称`
- **目的**：确保最终数据无重复

#### 第四层：最终清理
- **时机**：保存前最后一步
- **方法**：再次使用 `remove_duplicate_records()`
- **标识符**：`标题 + 发布时间 + 账号名称 + 链接`
- **目的**：彻底清理所有可能的重复

### 2. 数据更新策略

#### 新记录处理
- 完全新的记录直接添加
- 记录数量统计和日志输出

#### 现有记录更新
- 只更新数据字段：`阅读量`、`点赞量`、`评论量`
- 保持其他字段不变
- 更新操作日志记录

## 去重函数详解

### `remove_duplicate_records(df)`
```python
def remove_duplicate_records(df):
    """移除重复记录，基于标题+发布时间+账号名称+链接的组合"""
    if df.empty:
        return df
        
    # 创建唯一标识符
    df['unique_key'] = df['标题'] + '|' + df['发布时间'] + '|' + df['账号名称'] + '|' + df['链接']
    
    # 移除重复记录，保留最新的数据（基于索引）
    df = df.drop_duplicates(subset=['unique_key'], keep='last')
    
    # 移除临时列
    df = df.drop('unique_key', axis=1)
    
    return df
```

### `create_unique_id(df)`
```python
def create_unique_id(df):
    """创建统一的唯一标识符"""
    if df.empty:
        return df
    df['unique_id'] = df['标题'] + '|' + df['发布时间'] + '|' + df['账号名称']
    return df
```

## 处理流程

### 1. 数据获取阶段
```
获取新数据 → 内部去重 → 统计去重结果
```

### 2. 数据合并阶段
```
读取现有数据 → 创建统一标识符 → 识别新记录 → 识别更新记录 → 执行更新 → 合并数据
```

### 3. 数据保存阶段
```
最终去重 → 最终清理 → 排序 → 保存 → 统计结果
```

## 日志输出

系统提供详细的处理日志：

```
📊 获取到 X 条新数据
🔍 内部去重后剩余 X 条数据
📚 现有数据 X 条
➕ 发现 X 条新记录
🔄 发现 X 条需要更新的记录
✅ 已更新现有记录的数据
🔗 合并后共 X 条记录
🧹 最终去重：移除 X 条重复记录
🧽 最终清理：移除 X 条重复记录
💾 数据已保存到 [路径]，共 X 条记录
```

## 性能优化

### 1. 空数据检查
- 每个函数都检查空DataFrame
- 避免不必要的处理操作

### 2. 增量更新
- 只处理真正的新记录
- 只更新需要更新的字段

### 3. 内存管理
- 及时删除临时列
- 避免不必要的数据复制

## 错误处理

### 1. 文件不存在
- 自动创建空文件
- 初始化标准列结构

### 2. 数据格式错误
- 使用try-catch处理CSV读取错误
- 提供fallback机制

### 3. 空数据
- 优雅处理空数据集
- 提供有意义的日志信息

## 使用示例

### 基本使用
```python
from core.crawlers.toutiao_api import update_toutiao_publish_history

# 使用Cookie和URL更新数据
update_toutiao_publish_history(cookie_str="your_cookie", url="your_url")
```

### 只使用Cookie
```python
# 系统会自动选择对应的URL
update_toutiao_publish_history(cookie_str="your_cookie")
```

## 测试验证

### 重复运行测试
```bash
python core/crawlers/toutiao_api.py
```

预期结果：
- 第一次运行：添加新记录
- 后续运行：只更新现有记录，不添加重复数据

### 数据完整性检查
- 记录总数保持稳定
- 新记录正确添加
- 现有记录正确更新
- 无重复记录

## 配置选项

### 去重标识符
可以通过修改以下函数来调整去重策略：

1. **`remove_duplicate_records`**: 最严格的去重（包含链接）
2. **`create_unique_id`**: 标准去重（不包含链接）

### 更新字段
当前只更新数据字段，可以扩展：
```python
# 在update_toutiao_publish_history函数中
old_df.loc[mask, ['阅读量', '点赞量', '评论量']] = [
    new_row['阅读量'], 
    new_row['点赞量'], 
    new_row['评论量']
]
```

## 注意事项

1. **标识符一致性**: 确保所有去重函数使用相同的标识符逻辑
2. **数据完整性**: 去重过程中保持数据完整性
3. **性能考虑**: 大量数据时考虑分批处理
4. **日志监控**: 关注去重日志，及时发现异常

## 故障排除

### 常见问题

1. **仍然有重复数据**
   - 检查标识符逻辑是否一致
   - 确认所有去重步骤都执行了

2. **数据丢失**
   - 检查去重逻辑是否正确
   - 确认keep='last'参数

3. **性能问题**
   - 考虑优化标识符创建逻辑
   - 使用更高效的去重方法

### 调试方法

```python
# 启用详细日志
import logging
logging.basicConfig(level=logging.DEBUG)

# 检查去重前后数据量
print(f"去重前: {len(df)}")
df = remove_duplicate_records(df)
print(f"去重后: {len(df)}")
```

---

**最后更新**: 2025年9月13日
