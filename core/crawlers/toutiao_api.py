from datetime import datetime
from operator import index

import pandas as pd
import requests
import json
import os
import re
import urllib.parse

# è´¦å·IDåˆ°URLçš„æ˜ å°„å­—å…¸
url_dict = {
    "3484813672855172": "https://mp.toutiao.com/api/feed/mp_provider/v1/?provider_type=mp_provider&aid=13&app_name=news_article&category=mp_all&channel=&stream_api_version=88&genre_type_switch=%7B%22repost%22%3A1%2C%22small_video%22%3A1%2C%22toutiao_graphic%22%3A1%2C%22weitoutiao%22%3A1%2C%22xigua_video%22%3A1%7D&device_platform=pc&platform_id=0&visited_uid=3484813672855172&offset=0&count=100&keyword=&client_extra_params=%7B%22category%22%3A%22mp_all%22%2C%22real_app_id%22%3A%221231%22%2C%22need_forward%22%3A%22true%22%2C%22offset_mode%22%3A%221%22%2C%22page_index%22%3A%221%22%2C%22status%22%3A%228%22%2C%22source%22%3A%220%22%7D&app_id=1231",
    "4223685486980743":"https://mp.toutiao.com/api/feed/mp_provider/v1/?provider_type=mp_provider&aid=13&app_name=news_article&category=mp_all&channel=&stream_api_version=88&genre_type_switch=%7B%22repost%22%3A1%2C%22small_video%22%3A1%2C%22toutiao_graphic%22%3A1%2C%22weitoutiao%22%3A1%2C%22xigua_video%22%3A1%7D&device_platform=pc&platform_id=0&visited_uid=4223685486980743&offset=0&count=10&keyword=&client_extra_params=%7B%22category%22%3A%22mp_all%22%2C%22real_app_id%22%3A%221231%22%2C%22need_forward%22%3A%22true%22%2C%22offset_mode%22%3A%221%22%2C%22page_index%22%3A%221%22%2C%22status%22%3A%228%22%2C%22source%22%3A%220%22%7D&app_id=1231"
    # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šè´¦å·çš„URLæ˜ å°„
}

def build_toutiao_mp_url(visited_uid, offset=0, count=100, keyword=""):
    """
    æž„å»ºä»Šæ—¥å¤´æ¡åª’ä½“å¹³å°API URL

    Args:
        visited_uid (str): è¦è®¿é—®çš„è‡ªåª’ä½“è´¦å·UID
        offset (int): åˆ†é¡µåç§»é‡
        count (int): æ¯é¡µæ•°é‡
        keyword (str): æœç´¢å…³é”®è¯

    Returns:
        str: æž„å»ºå®Œæˆçš„URL
    """
    # åŸºç¡€URL
    base_url = "https://mp.toutiao.com/api/feed/mp_provider/v1/"

    # å›ºå®šå‚æ•°
    params = {
        "provider_type": "mp_provider",
        "aid": "13",
        "app_name": "news_article",
        "category": "mp_all",
        "channel": "",
        "stream_api_version": "88",
        "device_platform": "pc",
        "platform_id": "0",
        "visited_uid": visited_uid,
        "offset": str(offset),
        "count": str(count),
        "keyword": keyword,
        "app_id": "1231"
    }

    # JSONå‚æ•° - genre_type_switch
    genre_switch = {
        "repost": 1,
        "small_video": 1,
        "toutiao_graphic": 1,
        "weitoutiao": 1,
        "xigua_video": 1
    }
    params["genre_type_switch"] = json.dumps(genre_switch, ensure_ascii=False)

    # JSONå‚æ•° - client_extra_params
    client_params = {
        "category": "mp_all",
        "real_app_id": "1231",
        "need_forward": "true",
        "offset_mode": "1",
        "page_index": "1",
        "status": "8",
        "source": "0"
    }
    params["client_extra_params"] = json.dumps(client_params, ensure_ascii=False)

    # æž„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²
    query_string = urllib.parse.urlencode(params, quote_via=urllib.parse.quote)

    # ç»„åˆå®Œæ•´URL
    full_url = f"{base_url}?{query_string}"

    return full_url

def extract_account_id_from_cookie(cookie_str):
    """
    ä»ŽCookieå­—ç¬¦ä¸²ä¸­æå–è´¦å·ID
    
    Args:
        cookie_str: Cookieå­—ç¬¦ä¸²
        
    Returns:
        str: è´¦å·IDï¼Œå¦‚æžœæœªæ‰¾åˆ°åˆ™è¿”å›žNone
    """
    # æ¸…ç†Cookieå­—ç¬¦ä¸²
    cleaned_cookie = cookie_str.strip().replace('\n', '').replace('\r', '').replace('\t', '')
    
    # å°è¯•å¤šç§æ¨¡å¼æå–è´¦å·ID
    patterns = [
        r'uid_tt=([^;]+)',           # uid_tt=è´¦å·ID
        r'toutiao_sso_user=([^;]+)', # toutiao_sso_user=è´¦å·ID
        r'tt_webid=(\d+)',           # tt_webid=æ•°å­—ID
        r'sessionid=([^;]+)',        # sessionid=è´¦å·ID
    ]
    
    for pattern in patterns:
        match = re.search(pattern, cleaned_cookie)
        if match:
            account_id = match.group(1)
            print(f"ðŸ” ä»ŽCookieä¸­æå–åˆ°è´¦å·ID: {account_id}")
            return account_id
    
    print("âš ï¸ æœªèƒ½ä»ŽCookieä¸­æå–åˆ°è´¦å·ID")
    return None

def add_account_url(account_id, custom_url=None):
    """
    ä¸ºè´¦å·æ·»åŠ ä¸“ç”¨URL
    
    Args:
        account_id: è´¦å·ID
        custom_url: è‡ªå®šä¹‰URLï¼Œå¦‚æžœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ¨¡æ¿
    """
    if custom_url:
        url_dict[account_id] = custom_url
        print(f"âœ… ä¸ºè´¦å· {account_id} æ·»åŠ äº†è‡ªå®šä¹‰URL")
    else:
        # ä½¿ç”¨é»˜è®¤URLæ¨¡æ¿
        default_url = "https://mp.toutiao.com/api/feed/mp_provider/v1/?provider_type=mp_provider&aid=13&app_name=news_article&category=mp_all&channel=&stream_api_version=88&genre_type_switch=%7B%22repost%22%3A1%2C%22small_video%22%3A1%2C%22toutiao_graphic%22%3A1%2C%22weitoutiao%22%3A1%7D&device_platform=pc&platform_id=0&visited_uid={}&offset=0&count=100&keyword=&client_extra_params=%7B%22category%22%3A%22mp_all%22%2C%22real_app_id%22%3A%221231%22%2C%22need_forward%22%3A%22true%22%2C%22offset_mode%22%3A%221%22%2C%22page_index%22%3A%221%22%2C%22status%22%3A%228%22%2C%22source%22%3A%220%22%7D&app_id=1231"
        url_dict[account_id] = default_url.format(account_id)
        print(f"âœ… ä¸ºè´¦å· {account_id} æ·»åŠ äº†é»˜è®¤URLæ¨¡æ¿")

def get_url_for_account(account_id):
    """
    æ ¹æ®è´¦å·IDèŽ·å–å¯¹åº”çš„è¯·æ±‚URL
    
    Args:
        account_id: è´¦å·ID
        
    Returns:
        str: å¯¹åº”çš„URLï¼Œå¦‚æžœæœªæ‰¾åˆ°åˆ™è¿”å›žé»˜è®¤URL
    """
    if account_id in url_dict:
        print(f"âœ… æ‰¾åˆ°è´¦å· {account_id} çš„ä¸“ç”¨URL")
        return url_dict[account_id]
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°è´¦å· {account_id} çš„ä¸“ç”¨URLï¼Œä½¿ç”¨åŠ¨æ€æž„å»ºçš„URL")
        # ä½¿ç”¨æ–°çš„URLæž„å»ºå‡½æ•°
        return build_toutiao_mp_url(account_id)

def list_account_urls():
    """
    åˆ—å‡ºæ‰€æœ‰å·²é…ç½®çš„è´¦å·URL
    
    Returns:
        dict: è´¦å·IDåˆ°URLçš„æ˜ å°„
    """
    print("ðŸ“‹ å·²é…ç½®çš„è´¦å·URL:")
    for account_id, url in url_dict.items():
        print(f"  {account_id}: {url[:100]}...")
    return url_dict


def fetch_article_by_site(cookie_str,url=None):
    # æ¸…ç†Cookieå­—ç¬¦ä¸²ï¼Œç§»é™¤æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼
    cleaned_cookie = cookie_str.strip().replace('\n', '').replace('\r', '').replace('\t', '')
    
    # æå–è´¦å·ID
    account_id = extract_account_id_from_cookie(cleaned_cookie)
    
    # èŽ·å–å¯¹åº”çš„URL
    if not url:
        url = get_url_for_account(account_id)
    
    headers = {
        "accept": "application/json, text/plain, */*",
        "accept-language": "zh-CN,zh;q=0.9",
        "priority": "u=1, i",
        "referer": "https://mp.toutiao.com/profile_v4/manage/content/all",
        "rpc-persist-bytetim_business_stream_caller": "mp",
        "sec-ch-ua": '"Not;A=Brand";v="99", "Google Chrome";v="139", "Chromium";v="139"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"macOS"',
        "sec-fetch-dest": "empty",
        "sec-fetch-mode": "cors",
        "sec-fetch-site": "same-origin",
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36",
        "cookie": cleaned_cookie
    }
    try:
        resp = requests.get(url, headers=headers, timeout=30)
        
        # æ£€æŸ¥HTTPçŠ¶æ€ç 
        if resp.status_code == 401:
            raise Exception("Cookieå·²å¤±æ•ˆï¼Œéœ€è¦é‡æ–°ç™»å½• (401 Unauthorized)")
        elif resp.status_code == 403:
            raise Exception("Cookieå·²å¤±æ•ˆï¼Œæƒé™ä¸è¶³ (403 Forbidden)")
        elif resp.status_code == 429:
            raise Exception("è¯·æ±‚é¢‘çŽ‡è¿‡é«˜ï¼Œè¯·ç¨åŽå†è¯• (429 Too Many Requests)")
        elif resp.status_code != 200:
            raise Exception(f"HTTPè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {resp.status_code}")
        
        # æ£€æŸ¥å“åº”å†…å®¹
        try:
            resp_json = resp.json()
        except Exception as e:
            raise Exception(f"å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {str(e)}")
        
        # æ£€æŸ¥APIè¿”å›žçš„é”™è¯¯ä¿¡æ¯
        if 'error' in resp_json:
            error_msg = resp_json.get('error', '')
            if 'unauthorized' in error_msg.lower() or 'forbidden' in error_msg.lower():
                raise Exception("Cookieå·²å¤±æ•ˆï¼Œéœ€è¦é‡æ–°ç™»å½•")
            elif 'rate limit' in error_msg.lower() or 'too many requests' in error_msg.lower():
                raise Exception("è¯·æ±‚é¢‘çŽ‡è¿‡é«˜ï¼Œè¯·ç¨åŽå†è¯•")
            else:
                raise Exception(f"APIè¿”å›žé”™è¯¯: {error_msg}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®è¿”å›ž
        if 'data' not in resp_json:
            raise Exception("APIå“åº”ä¸­æ²¡æœ‰æ•°æ®å­—æ®µ")
        
        articles = resp_json.get("data", {})
        if not articles:
            raise Exception("APIè¿”å›žçš„æ–‡ç« æ•°æ®ä¸ºç©º")
            
    except requests.exceptions.Timeout:
        raise Exception("è¯·æ±‚è¶…æ—¶ï¼Œç½‘ç»œè¿žæŽ¥ç¼“æ…¢")
    except requests.exceptions.ConnectionError:
        raise Exception("ç½‘ç»œè¿žæŽ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ")
    except Exception as e:
        # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä¿æŒåŽŸæœ‰çš„é”™è¯¯ä¿¡æ¯
        raise e
    df = pd.DataFrame(columns='æ ‡é¢˜ å‘å¸ƒæ—¶é—´ å±•çŽ°é‡ é˜…è¯»é‡ ç‚¹èµžé‡ è¯„è®ºé‡ è´¦å·åç§° é“¾æŽ¥'.split(" "))
    articles = resp_json.get("data", {})
    processed_titles = set()  # ç”¨äºŽè·Ÿè¸ªå·²å¤„ç†çš„æ ‡é¢˜
    
    for article in articles:
        article = article.get("assembleCell", {})
        title = article['itemCell']['articleBase']['title']
        if len(title) > 64:
            title = title[:64] + "..."
        title = title.replace("\n", "").strip()
        
        publishTime = article['itemCell']['articleBase']['publishTime']
        publishTime = datetime.fromtimestamp(publishTime)
        if publishTime.year < 2025:
            continue
            
        article_url = article['itemCell']['articleBase']['articleURL']
        
        # æ–‡ç« æ•°æ®
        article_status = json.loads(article['itemCell']['extra']['origin_content'])['ArticleStat']
        comment_count = article_status['CommentCount']
        like_count = article_status['DiggCount']
        impression_count = article_status['ImpressionCount']
        read_count = article_status['GoDetailCount']

        article_type = json.loads(article['itemCell']['extra']['origin_content'])['ArticleAttr']['ArticleType']
        article_type_suffix = {
            "article": "",
            "weitoutiao": "å¾®å¤´æ¡",
            "video": "è§†é¢‘",
            "short_video": "è§†é¢‘",
        }.get(article_type, "æœªçŸ¥ç±»åž‹")

        # åˆ›å»ºå¸¦åŽç¼€çš„å®Œæ•´æ ‡é¢˜
        full_title = title + article_type_suffix if article_type_suffix else title
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡ç›¸åŒçš„æ ‡é¢˜+URLç»„åˆ
        title_url_key = f"{full_title}|{article_url}"
        if title_url_key in processed_titles:
            continue
        processed_titles.add(title_url_key)
        
        author = "å¤´æ¡å·-"+json.loads(article['itemCell']['extra']['origin_content'])['ArticleAttr']['Extra']['user_name']
        publishTime_str = publishTime.strftime('%Y-%m-%d %H:%M:%S')

        df = df._append({
            "æ ‡é¢˜": full_title,
            "å‘å¸ƒæ—¶é—´": publishTime_str,
            "å±•çŽ°é‡": impression_count,
            "é˜…è¯»é‡": read_count,
            "ç‚¹èµžé‡": like_count,
            "è¯„è®ºé‡": comment_count,
            "è´¦å·åç§°": author,
            "é“¾æŽ¥": article_url,
        }, ignore_index=True)

    return df

def remove_duplicate_records(df):
    """ç§»é™¤é‡å¤è®°å½•ï¼ŒåŸºäºŽæ ‡é¢˜+å‘å¸ƒæ—¶é—´+è´¦å·åç§°+é“¾æŽ¥çš„ç»„åˆ"""
    if df.empty:
        return df
        
    # åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦
    df['unique_key'] = df['æ ‡é¢˜'] + '|' + df['å‘å¸ƒæ—¶é—´'] + '|' + df['è´¦å·åç§°'] + '|' + df['é“¾æŽ¥']
    
    # ç§»é™¤é‡å¤è®°å½•ï¼Œä¿ç•™æœ€æ–°çš„æ•°æ®ï¼ˆåŸºäºŽç´¢å¼•ï¼‰
    df = df.drop_duplicates(subset=['unique_key'], keep='last')
    
    # ç§»é™¤ä¸´æ—¶åˆ—
    df = df.drop('unique_key', axis=1)
    
    return df

def create_unique_id(df):
    """åˆ›å»ºç»Ÿä¸€çš„å”¯ä¸€æ ‡è¯†ç¬¦"""
    if df.empty:
        return df
    df['unique_id'] = df['æ ‡é¢˜'] + '|' + df['å‘å¸ƒæ—¶é—´'] + '|' + df['è´¦å·åç§°']
    return df

def update_toutiao_publish_history(cookie_str=None,url=None):
    """æ›´æ–°ä»Šæ—¥å¤´æ¡å‘å¸ƒåŽ†å²æ•°æ®"""
    import os
    filepath = os.path.join(
        os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
        "workspace",
        "data",
        "publish_history_for_calendar.csv"
    )
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(filepath), exist_ok=True)

    # å¦‚æžœæ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œåˆ™åˆå§‹åŒ–
    if not os.path.exists(filepath) or os.path.getsize(filepath) == 0:
        columns = ["æ ‡é¢˜", "è´¦å·åç§°", "å‘å¸ƒæ—¶é—´", "é˜…è¯»é‡", "ç‚¹èµžé‡", "è¯„è®ºé‡", "é“¾æŽ¥"]
        empty_df = pd.DataFrame(columns=columns)
        empty_df.to_csv(filepath, index=False, encoding="utf-8-sig")

    # èŽ·å–ä»Šæ—¥å¤´æ¡æ–‡ç« æ•°æ®
    toutiao_df = fetch_article_by_site(cookie_str,url)
    
    if toutiao_df.empty:
        print("âš ï¸ æœªèŽ·å–åˆ°æ–°æ•°æ®")
        return True

    # åªä¿ç•™éœ€è¦çš„å­—æ®µï¼Œç¡®ä¿å­—æ®µé¡ºåºä¸€è‡´
    columns = ["æ ‡é¢˜", "è´¦å·åç§°", "å‘å¸ƒæ—¶é—´", "é˜…è¯»é‡", "ç‚¹èµžé‡", "è¯„è®ºé‡", "é“¾æŽ¥"]
    toutiao_df = toutiao_df[columns]
    
    # 1. å…ˆå¯¹æ–°æ•°æ®è¿›è¡Œå†…éƒ¨åŽ»é‡
    print(f"ðŸ“Š èŽ·å–åˆ° {len(toutiao_df)} æ¡æ–°æ•°æ®")
    toutiao_df = remove_duplicate_records(toutiao_df)
    print(f"ðŸ” å†…éƒ¨åŽ»é‡åŽå‰©ä½™ {len(toutiao_df)} æ¡æ•°æ®")

    # è¯»å–çŽ°æœ‰æ•°æ®
    try:
        old_df = pd.read_csv(filepath, encoding="utf-8-sig")
        if old_df.empty:
            print("ðŸ“ çŽ°æœ‰æ•°æ®ä¸ºç©ºï¼Œç›´æŽ¥ä¿å­˜æ–°æ•°æ®")
            combined_df = toutiao_df
        else:
            print(f"ðŸ“š çŽ°æœ‰æ•°æ® {len(old_df)} æ¡")
            
            # 2. ä¸ºä¸¤ä¸ªæ•°æ®é›†åˆ›å»ºç»Ÿä¸€çš„å”¯ä¸€æ ‡è¯†ç¬¦
            old_df = create_unique_id(old_df)
            toutiao_df = create_unique_id(toutiao_df)
            
            # 3. æ‰¾å‡ºéœ€è¦æ–°å¢žçš„è®°å½•ï¼ˆunique_idä¸åœ¨æ—§æ•°æ®ä¸­çš„ï¼‰
            new_records = toutiao_df[~toutiao_df['unique_id'].isin(old_df['unique_id'])]
            print(f"âž• å‘çŽ° {len(new_records)} æ¡æ–°è®°å½•")
            
            # 4. æ‰¾å‡ºéœ€è¦æ›´æ–°çš„è®°å½•ï¼ˆunique_idåœ¨æ—§æ•°æ®ä¸­çš„ï¼‰
            existing_records = toutiao_df[toutiao_df['unique_id'].isin(old_df['unique_id'])]
            print(f"ðŸ”„ å‘çŽ° {len(existing_records)} æ¡éœ€è¦æ›´æ–°çš„è®°å½•")
            
            if not existing_records.empty:
                # æ›´æ–°å·²å­˜åœ¨çš„è®°å½•
                for _, new_row in existing_records.iterrows():
                    mask = old_df['unique_id'] == new_row['unique_id']
                    old_df.loc[mask, ['é˜…è¯»é‡', 'ç‚¹èµžé‡', 'è¯„è®ºé‡']] = [
                        new_row['é˜…è¯»é‡'], 
                        new_row['ç‚¹èµžé‡'], 
                        new_row['è¯„è®ºé‡']
                    ]
                print("âœ… å·²æ›´æ–°çŽ°æœ‰è®°å½•çš„æ•°æ®")
            
            # 5. åˆå¹¶æ–°è®°å½•å’Œæ›´æ–°åŽçš„æ—§è®°å½•
            if not new_records.empty:
                # ç§»é™¤unique_idåˆ—ï¼Œä¿æŒåŽŸæœ‰æ ¼å¼
                new_records = new_records.drop('unique_id', axis=1)
                old_df = old_df.drop('unique_id', axis=1)
                combined_df = pd.concat([old_df, new_records], ignore_index=True)
                print(f"ðŸ”— åˆå¹¶åŽå…± {len(combined_df)} æ¡è®°å½•")
            else:
                combined_df = old_df.drop('unique_id', axis=1)
                print("â„¹ï¸ æ²¡æœ‰æ–°è®°å½•éœ€è¦æ·»åŠ ")
                
    except pd.errors.EmptyDataError:
        print("ðŸ“ çŽ°æœ‰æ•°æ®ä¸ºç©ºï¼Œç›´æŽ¥ä¿å­˜æ–°æ•°æ®")
        combined_df = toutiao_df

    # 6. æœ€ç»ˆåŽ»é‡ï¼šåŸºäºŽæ ‡é¢˜+å‘å¸ƒæ—¶é—´+è´¦å·åç§°
    before_dedup = len(combined_df)
    combined_df = combined_df.drop_duplicates(subset=['æ ‡é¢˜', 'å‘å¸ƒæ—¶é—´', 'è´¦å·åç§°'], keep='last')
    after_dedup = len(combined_df)
    
    if before_dedup != after_dedup:
        print(f"ðŸ§¹ æœ€ç»ˆåŽ»é‡ï¼šç§»é™¤ {before_dedup - after_dedup} æ¡é‡å¤è®°å½•")
    
    # 7. ä½¿ç”¨è‡ªå®šä¹‰åŽ»é‡å‡½æ•°è¿›è¡Œæœ€ç»ˆæ¸…ç†
    before_final_dedup = len(combined_df)
    combined_df = remove_duplicate_records(combined_df)
    after_final_dedup = len(combined_df)
    
    if before_final_dedup != after_final_dedup:
        print(f"ðŸ§½ æœ€ç»ˆæ¸…ç†ï¼šç§»é™¤ {before_final_dedup - after_final_dedup} æ¡é‡å¤è®°å½•")
    
    # 8. æŒ‰å‘å¸ƒæ—¶é—´æŽ’åº
    combined_df['å‘å¸ƒæ—¶é—´'] = pd.to_datetime(combined_df['å‘å¸ƒæ—¶é—´'])
    combined_df = combined_df.sort_values('å‘å¸ƒæ—¶é—´', ascending=False)
    combined_df['å‘å¸ƒæ—¶é—´'] = combined_df['å‘å¸ƒæ—¶é—´'].dt.strftime('%Y-%m-%d %H:%M:%S')
    
    # 9. ä¿å­˜æ•°æ®
    combined_df.to_csv(filepath, index=False, encoding="utf-8-sig")
    print(f"ðŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° {filepath}ï¼Œå…± {len(combined_df)} æ¡è®°å½•")
    return True

if __name__ == "__main__":
    update_toutiao_publish_history(
        cookie_str="""
tt_webid=7521987389596075570; _ga=GA1.1.1598848217.1753344798; is_staff_user=false; gfkadpd=1231,25897; ttcid=328c07b7ca7c49bf8dc07f5eddc200b125; csrf_session_id=df0622c6faddc708a0ebc87ea42d1342; s_v_web_id=verify_mf3qbrr8_8Yy5ahpv_PmU7_4Bo5_8ebj_o7U4OodlpWeY; tt_scid=N8ppakwP24pl6tyEd5cvtYFf-hPiZ1m4s6l-0.1Pnxo134zkg9ZHJ-HOGbdHI6mcfd80; passport_csrf_token=20061e7643ebbeca095c098f9276f406; passport_csrf_token_default=20061e7643ebbeca095c098f9276f406; _ga_QEHZPBE5HH=GS2.1.s1756970354$o12$g1$t1756971985$j60$l0$h0; passport_mfa_token=CjAJWSMZ8Odp3cOdicWpD%2BcuYeo7adS3e5g3WkGj3fAI2UvJ9bX0QqxFFHb9UYuahocaSgo8AAAAAAAAAAAAAE9v4oxaVwcYyP3PQpB7RnmsK46mtIqPYs0H90frrI1JwMEf2XvAni6MDPkuYwXpOMAJEJyk%2Bw0Y9rHRbCACIgEDutBNXA%3D%3D; d_ticket=7f084b5eaef84e7998eec8c9bf5f9bb52680a; n_mh=Ibfguk1p9RJdSVK2ychBzXbStl9jDnK1HF9SpJ9JK5U; sso_uid_tt=4f4b5c304b60b8a772139d25e83c2a10; sso_uid_tt_ss=4f4b5c304b60b8a772139d25e83c2a10; toutiao_sso_user=631365540fee7f96a161c0905d74f3d9; toutiao_sso_user_ss=631365540fee7f96a161c0905d74f3d9; sid_ucp_sso_v1=1.0.0-KDM4ZDY0YmEyYjZjNTVhNWUzYmMyMTdiNGJmNTIzOGFjMzM5Mjk1NjAKHgjrsZC23vV4EO2H5cUGGM8JIAww7tn55gU4AkDvBxoCaGwiIDYzMTM2NTU0MGZlZTdmOTZhMTYxYzA5MDVkNzRmM2Q5; ssid_ucp_sso_v1=1.0.0-KDM4ZDY0YmEyYjZjNTVhNWUzYmMyMTdiNGJmNTIzOGFjMzM5Mjk1NjAKHgjrsZC23vV4EO2H5cUGGM8JIAww7tn55gU4AkDvBxoCaGwiIDYzMTM2NTU0MGZlZTdmOTZhMTYxYzA5MDVkNzRmM2Q5; odin_tt=e52e06d6f667fba5b9c9195e41472a4d37e2d7d6e875a7d70f97b6e99973d8f4d15ee773c5f3360c007ffdd4d469d0b267481c7ecf8a2e797e91624a847ab4af; sid_guard=2cd96b914ad93c4827a5957a99d0362a%7C1756972014%7C5184001%7CMon%2C+03-Nov-2025+07%3A46%3A55+GMT; uid_tt=4b5d9d0a61563c9ce3c1f741a22d9c85; uid_tt_ss=4b5d9d0a61563c9ce3c1f741a22d9c85; sid_tt=2cd96b914ad93c4827a5957a99d0362a; sessionid=2cd96b914ad93c4827a5957a99d0362a; sessionid_ss=2cd96b914ad93c4827a5957a99d0362a; session_tlb_tag=sttt%7C7%7CLNlrkUrZPEgnpZV6mdA2Kv________-tuaz-kb3hRMQNVsbHi0jcZlEMHv9oq6P-IdBOv0ClK-U%3D; sid_ucp_v1=1.0.0-KDczNDcxN2NjZGM2YjcyYjc1OWQxYzZiMDJlMzM4NmVlNDIxMTVhOGIKGAjrsZC23vV4EO6H5cUGGM8JIAw4AkDvBxoCbHEiIDJjZDk2YjkxNGFkOTNjNDgyN2E1OTU3YTk5ZDAzNjJh; ssid_ucp_v1=1.0.0-KDczNDcxN2NjZGM2YjcyYjc1OWQxYzZiMDJlMzM4NmVlNDIxMTVhOGIKGAjrsZC23vV4EO6H5cUGGM8JIAw4AkDvBxoCbHEiIDJjZDk2YjkxNGFkOTNjNDgyN2E1OTU3YTk5ZDAzNjJh; ttwid=1%7CFIb8XOAcosIceiGnsMHDreVkYcf6QSlkstXTMLOl7ZQ%7C1756972015%7C39e6ead5243e77421496530538191109848029ea7045a1eabd293a816eb7b121
        """
        ,url="""
https://mp.toutiao.com/api/feed/mp_provider/v1/?provider_type=mp_provider&aid=13&app_name=news_article&category=mp_all&channel=&stream_api_version=88&genre_type_switch=%7B%22repost%22%3A1%2C%22small_video%22%3A1%2C%22toutiao_graphic%22%3A1%2C%22weitoutiao%22%3A1%2C%22xigua_video%22%3A1%7D&device_platform=pc&platform_id=0&visited_uid=531811017169131&offset=0&count=10&keyword=&client_extra_params=%7B%22category%22%3A%22mp_all%22%2C%22real_app_id%22%3A%221231%22%2C%22need_forward%22%3A%22true%22%2C%22offset_mode%22%3A%221%22%2C%22page_index%22%3A%221%22%2C%22status%22%3A%228%22%2C%22source%22%3A%220%22%7D&app_id=1231
        """
        )